{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75f8d8c7-ad9a-4bb6-804d-a7da4908af7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "096f0332-8fa2-4e9b-8ab6-c3260c5e8465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1a9ff421470>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 5 # 设置总共需要训练多少轮\n",
    "batch_size_train = 32# 每次训练mini_batch的大小\n",
    "batch_size_test = 100000 # 每次测试所用样本的大小\n",
    "# learning_rate = 0.0011 # 学习率\n",
    "# learning_rate = 0.0005\n",
    "learning_rate = 0.01\n",
    "log_interval = 256 # 按照mini_batch的大小打印测试结果\n",
    "random_seed = 1 # 将随机种子设置为 1。可以根据需要选择任何整数值作为随机种子。\n",
    "torch.manual_seed(random_seed) # 在使用相同的随机种子、相同的环境和相同的代码时，能保证得到相同的随机数序列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "671670f8-cfcc-4d59-9b1c-a500e03ca3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 残差模块\n",
    "# class ResidualBlock(torch.nn.Module):\n",
    "#     #继承自ResidualBlock，传入输出通道数，输出通道数，滑动卷积核的步长\n",
    "#     def __init__(self, in_channels, out_channels, stride=1):\n",
    "#         super(ResidualBlock, self).__init__()\n",
    "        \n",
    "#         #采用3x3卷积核进行卷积，同时注意进行填充以保持图像尺寸不变\n",
    "#         self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
    "#                                 stride=stride, padding=1, bias=False)\n",
    "#         #激活函数采用RELU\n",
    "#         self.relu = nn.ReLU()\n",
    "        \n",
    "#     #正向传播，覆写torch.nn.Module中的魔法方法\n",
    "#     def forward(self,x):\n",
    "#         # 保存输入数据，采用恒等映射\n",
    "#         identity = x\n",
    "        \n",
    "#         #进行卷积和激活\n",
    "#         out =self.conv1(x)\n",
    "        \n",
    "#         out =self.relu(out)\n",
    "\n",
    "#         # 还原结果\n",
    "#         out = out + x\n",
    "        \n",
    "#         out = self.relu(out)\n",
    "#         # 返回结果\n",
    "#         return out\n",
    "# # 初代\n",
    "# # 构建包含ResidualBlock的CNN\n",
    "# class ResNet_CNN(nn.Module):\n",
    "#     def __init__(self,num_classes=10):\n",
    "#         super(ResNet_CNN,self).__init__()\n",
    "        \n",
    "#         self.conv1 = nn.Conv2d(1, 16, kernel_size=5, padding=1)\n",
    "#         self.conv2 = nn.Conv2d(16, 32, kernel_size=5, padding=1)\n",
    "#         self.relu = nn.ReLU()\n",
    "\n",
    "#         self.res1 = ResidualBlock(16, 16, stride=1)\n",
    "#         self.res2 = ResidualBlock(32, 32, stride=1)\n",
    "        \n",
    "#         self.mp = nn.MaxPool2d(2)\n",
    "        \n",
    "#         # 用一个自适应均值池化层将每个通道维度变成1*1，此句可选\n",
    "#         #self.avg_pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        \n",
    "#         self.fc = nn.Linear(800, 10)\n",
    "#         # self.fc = nn.Linear(num_channels * height * width, num_classes)\n",
    "#         # MINIST数据集中的图像，通常的高度和宽度是28x28像素。\n",
    "#     def forward(self,x):\n",
    "#         in_size = x.size(0)\n",
    " \n",
    "#         x = self.mp(F.relu(self.conv1(x)))\n",
    "#         x = self.res1(x)\n",
    "#         x = self.mp(F.relu(self.conv2(x)))\n",
    "#         x = self.res2(x)\n",
    " \n",
    "#         x = x.view(in_size, -1)\n",
    "#         x = self.fc(x)\n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b2fdec5-ec91-49f1-a4fe-e3ba9eac6f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # \n",
    "# class ResidualBlock(nn.Module):\n",
    "#     # Residual Block需要保证输出和输入通道数x一样\n",
    "#     def __init__(self, channels):\n",
    "#         super(ResidualBlock, self).__init__()\n",
    "#         self.channels = channels\n",
    "#         # 3*3卷积核，保证图像大小不变将padding设为1\n",
    "#         # 第一个卷积\n",
    "#         self.conv1 = nn.Conv2d(channels, channels,\n",
    "#                                kernel_size=3, padding=1)\n",
    "#         # 第二个卷积\n",
    "#         self.conv2 = nn.Conv2d(channels, channels,\n",
    "#                                kernel_size=3, padding=1)\n",
    "#         #第三个卷积，1*1\n",
    "#         self.conv3 = nn.Conv2d(channels, channels, kernel_size=1)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         # 激活\n",
    "#         y = F.relu(self.conv1(x))\n",
    "#         y = self.conv2(y)\n",
    "#         # 先求和 后激活\n",
    "#         z = self.conv3(x)\n",
    "#         return F.relu(z + y)\n",
    "# # 构建包含ResidualBlock的CNN\n",
    "# class ResNet_CNN(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(ResNet_CNN, self).__init__()\n",
    "#         #卷积层\n",
    "#         self.conv1 = nn.Conv2d(1 ,32, kernel_size=3,padding=1)\n",
    "#         self.conv2 = nn.Conv2d(32 ,64, kernel_size=3,padding=1)\n",
    "#         self.conv3 = nn.Conv2d(64 ,32,kernel_size=3,padding=1)\n",
    "\n",
    "#         #残差神经网络层，其中已经包含了relu\n",
    "#         self.rblock1 = ResidualBlock(32)\n",
    "#         self.rblock2 = ResidualBlock(64)\n",
    "#         self.rblock3 = ResidualBlock(32)\n",
    "        \n",
    "#         #BN层，归一化，使数据在进行Relu之前不会因为数据过大而导致网络性能的不稳定\n",
    "#         self.bn1 = nn.BatchNorm2d(32)\n",
    "#         self.bn2 = nn.BatchNorm2d(64)\n",
    "#         self.bn3 = nn.BatchNorm2d(32)\n",
    "        \n",
    "#         #最大池化，一般最大池化效果都比平均池化好些\n",
    "#         self.mp = nn.MaxPool2d(2)\n",
    "        \n",
    "#         #fully connectected全连接层\n",
    "#         self.fc1 = nn.Linear(32*3*3, 256)  # 线性\n",
    "#         self.fc2 = nn.Linear(256, 64)\n",
    "        \n",
    "#         self.fc6 = nn.Linear(64, 10)  # 线性\n",
    "#     def forward(self, x):\n",
    "#         in_size = x.size(0)\n",
    "        \n",
    "#         x = self.conv1(x)   #channels:1-32  w*h:28*28  \n",
    "#         # x = self.bn1(x)\n",
    "#         x = self.mp(F.relu(x)) \n",
    "#         x = self.rblock1(x)\n",
    "        \n",
    "#         x = self.conv2(x)   #channels:32-64 w*h:14*14\n",
    "#         x = self.mp(F.relu(x))\n",
    "#         # x = self.bn2(x)\n",
    "#         x = self.rblock2(x)\n",
    "        \n",
    "#         # x = self.mp(x)      #最大池化,channels:64-64    w*h:28*28->14*14\n",
    "        \n",
    "#         x = self.conv3(x)   #channels:64-32    w*h:7*7\n",
    "#         # x = self.bn3(x)\n",
    "#         x = self.mp(F.relu(x))\n",
    "#         x = self.rblock3(x)\n",
    "        \n",
    "        \n",
    "#         # x = self.mp(x)      #最大池化,channels:192-192  w*h:14*14->7*7\n",
    "\n",
    "#         x = x.view(in_size, -1)     #展开成向量\n",
    "#         x = F.relu(self.fc1(x))  # 使用relu函数来激活\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         return self.fc6(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a51fbed-4f98-49d8-96b9-74ca30064c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ResidualBlock(nn.Module):\n",
    "#     def __init__(self, channels):\n",
    "#         super(ResidualBlock, self).__init__()\n",
    "#         self.channels = channels\n",
    "#         self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "#         self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    " \n",
    "#     def forward(self, x):\n",
    "#         y = F.relu(self.conv1(x))\n",
    "#         y = self.conv2(y)\n",
    "#         return F.relu(x + y)\n",
    " \n",
    "# class Net(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Net, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(1, 16, kernel_size=5)\n",
    "#         self.conv2 = nn.Conv2d(16, 32, kernel_size=3) # 88 = 24x3 + 16\n",
    " \n",
    "#         self.rblock1 = ResidualBlock(16)\n",
    "#         self.rblock2 = ResidualBlock(32)\n",
    "#         self.bn1 = nn.BatchNorm2d(16)\n",
    "#         self.bn2 = nn.BatchNorm2d(32)\n",
    "        \n",
    "#         self.mp = nn.MaxPool2d(2)\n",
    "#         self.fc_1 = nn.Linear(800, 64) # 暂时不知道1408咋能自动出来的\n",
    "#         self.fc_2 = nn.Linear(64, 10)\n",
    " \n",
    "#     def forward(self, x):\n",
    "#         in_size = x.size(0)\n",
    " \n",
    "#         x = self.mp(F.relu(self.conv1(x)))\n",
    "#         x = self.rblock1(x)\n",
    "#         x = self.bn1(x)\n",
    "#         x = self.mp(F.relu(self.conv2(x)))\n",
    "#         x = self.rblock2(x)\n",
    "#         x = self.bn2(x)\n",
    "#         x = x.view(in_size, -1)\n",
    "#         x = self.fc_1(x)\n",
    "#         x = self.fc_2(x)\n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d019137-1bc3-435c-b38f-fcf8c9bae912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# torch.cuda.empty_cache()\n",
    "# model = Net().to(device)\n",
    "# # model = ResNet_CNN().to(device)\n",
    "# loss_f = nn.CrossEntropyLoss()# 自带softmax\n",
    "# # loss_f = nn.NLLLoss()\n",
    "# optimizer = optim.SGD(model.parameters(),lr=learning_rate,  momentum=0.5)\n",
    "# # 使用StepLR策略\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "# optimizer = optim.Adam(model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7c49bb9e-d51f-4fc8-b6a8-ed2a442f9ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(Net, self).__init__()\n",
    "        self.in_channels = 16\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.layer1 = self._make_layer(16, 2, stride=1)\n",
    "        self.layer2 = self._make_layer(32, 2, stride=2)\n",
    "        self.layer3 = self._make_layer(64, 2, stride=2)\n",
    "        self.linear = nn.Linear(64, num_classes)\n",
    "\n",
    "    def _make_layer(self, out_channels, blocks, stride):\n",
    "        strides = [stride] + [1]*(blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(ResidualBlock(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.avg_pool2d(out, out.size()[3])\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.empty_cache()\n",
    "model = Net().to(device)\n",
    "loss_f = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "138ff271-b191-4386-94ca-e2b7a234cc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ResidualBlock(nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels, stride=1):\n",
    "#         super(ResidualBlock, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "#         self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "#         self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "#         self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "#         self.shortcut = nn.Sequential()\n",
    "#         if stride != 1 or in_channels != out_channels:\n",
    "#             self.shortcut = nn.Sequential(\n",
    "#                 nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "#                 nn.BatchNorm2d(out_channels)\n",
    "#             )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         out = F.relu(self.bn1(self.conv1(x)))\n",
    "#         out = self.bn2(self.conv2(out))\n",
    "#         out += self.shortcut(x)\n",
    "#         out = F.relu(out)\n",
    "#         return out\n",
    "\n",
    "# class Net(nn.Module):\n",
    "#     def __init__(self, num_classes=10):\n",
    "#         super(Net, self).__init__()\n",
    "#         self.in_channels = 16\n",
    "        \n",
    "#         self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "#         self.bn1 = nn.BatchNorm2d(16)\n",
    "#         self.layer1 = self._make_layer(16, 2, stride=1)\n",
    "#         self.layer2 = self._make_layer(32, 2, stride=2)\n",
    "#         self.layer3 = self._make_layer(64, 2, stride=2)\n",
    "#         self.linear = nn.Linear(64, num_classes)\n",
    "\n",
    "#     def _make_layer(self, out_channels, blocks, stride):\n",
    "#         strides = [stride] + [1]*(blocks-1)\n",
    "#         layers = []\n",
    "#         for stride in strides:\n",
    "#             layers.append(ResidualBlock(self.in_channels, out_channels, stride))\n",
    "#             self.in_channels = out_channels\n",
    "#         return nn.Sequential(*layers)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         out = F.relu(self.bn1(self.conv1(x)))\n",
    "#         out = self.layer1(out)\n",
    "#         out = self.layer2(out)\n",
    "#         out = self.layer3(out)\n",
    "#         out = F.avg_pool2d(out, (out.size()[2], out.size()[3]))    # 修改此处\n",
    "#         out = out.view(out.size(0), -1)\n",
    "#         out = self.linear(out)\n",
    "#         return out\n",
    "\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# torch.cuda.empty_cache()\n",
    "# model = Net().to(device)\n",
    "# loss_f = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b36b3981-f9cc-410d-a7f6-22d7e0c896d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ResidualBlock(nn.Module):\n",
    "#     def __init__(self, channels):\n",
    "#         super(ResidualBlock, self).__init__()\n",
    "#         self.channels = channels\n",
    "#         self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "#         self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "#         self.bn1 = nn.BatchNorm2d(channels) # Add BatchNorm layer\n",
    "#         self.bn2 = nn.BatchNorm2d(channels) # Add BatchNorm layer\n",
    " \n",
    "#     def forward(self, x):\n",
    "#         y = F.relu(self.bn1(self.conv1(x)))\n",
    "#         y = self.bn2(self.conv2(y))\n",
    "#         return F.relu(x + y)\n",
    " \n",
    "# class Net(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Net, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(1, 16, kernel_size=5)\n",
    "#         self.conv2 = nn.Conv2d(16, 32, kernel_size=3)\n",
    "\n",
    "#         self.rblock1 = ResidualBlock(16)\n",
    "#         self.rblock2 = ResidualBlock(32)\n",
    "\n",
    "#         self.bn1 = nn.BatchNorm2d(16)\n",
    "#         self.bn2 = nn.BatchNorm2d(32)\n",
    "\n",
    "#         self.mp = nn.MaxPool2d(2)\n",
    "\n",
    "#         self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "#         self.fc_1 = nn.Linear(800, 128) # Increase number of output features\n",
    "#         self.fc_2 = nn.Linear(128, 64) # New hidden layer\n",
    "#         self.fc_3 = nn.Linear(64, 10)\n",
    " \n",
    "#     def forward(self, x):\n",
    "#         in_size = x.size(0)\n",
    "\n",
    "#         x = self.mp(F.relu(self.bn1(self.conv1(x))))\n",
    "#         x = self.rblock1(x)\n",
    "        \n",
    "#         x = self.mp(F.relu(self.bn2(self.conv2(x))))\n",
    "#         x = self.rblock2(x)\n",
    "        \n",
    "#         x = x.view(in_size, -1)\n",
    "#         x = self.dropout(F.relu(self.fc_1(x)))\n",
    "\n",
    "#         x = self.dropout(F.relu(self.fc_2(x)))\n",
    "        \n",
    "#         x = self.fc_3(x)\n",
    "#         return x\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# torch.cuda.empty_cache()\n",
    "# model = Net().to(device)\n",
    "# loss_f = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7e85b137-166c-42db-a654-c303cbc77eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 调用GPU\n",
    "# torch.backends.cudnn.benchmark = True\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# #print(device)\n",
    "# torch.cuda.empty_cache()\n",
    "# model = ResNet_CNN().to(device)\n",
    "# model.to(device)\n",
    "\n",
    "# # 构建损失函数\n",
    "# loss_f = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# # 构建优化器\n",
    "# optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.5)\n",
    "\n",
    "# # 创建学习率调度器\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2, threshold=0.00005, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08)\n",
    "\n",
    "# # 进行一次优化器的step操作\n",
    "# optimizer.step()\n",
    "\n",
    "# # 模拟一个评估指标（metrics）\n",
    "# metrics = 0.8\n",
    "\n",
    "# # 更新学习率调度器的状态\n",
    "# scheduler.step(metrics)\n",
    "\n",
    "# # 获取最新的学习率\n",
    "# learning_rate = optimizer.param_groups[0]['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a4afe0da-3224-499a-814f-1b737f7150f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.MNIST('./data/', train=True, download=True,\n",
    "                                transform=torchvision.transforms.Compose([\n",
    "                                torchvision.transforms.ToTensor(),\n",
    "                                torchvision.transforms.Normalize(\n",
    "                                (0.1307,), (0.3081,))\n",
    "                                ])),\n",
    "    batch_size=batch_size_train, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fe71dc97-881e-4f74-97b4-f51fe496b00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练模型\n",
    "def train(epochs):\n",
    "    for epoch in range(epochs):\n",
    "        model.train() # 让模型进入训练模式\n",
    "        for batch_idx,(data,target) in enumerate(train_loader):\n",
    "            data,target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = loss_f(output,target)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % log_interval == 0:\n",
    "                print('Train Epoch:{} [{}/{} ({:.0f}%)]\\tLoss:{:.6f}'.format(\n",
    "                    epoch,batch_idx*len(data),len(train_loader.dataset),\n",
    "                    100.*batch_idx/len(train_loader),loss.item()\n",
    "                ))\n",
    "        # 每个epoch结束后更新学习率\n",
    "        scheduler.step()\n",
    "                #epoch表示当前的训练轮数。\n",
    "                #batch_idx*len(data)表示当前处理过的训练样本数量。\n",
    "                #len(train_loader.dataset)是训练集中的总样本数。\n",
    "                #100.*batch_idx/len(train_loader)计算了目前训练的进度（百分比形式）。\n",
    "                #loss.item()则展示了最新批次的训练损失。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6f345ab9-9d35-485e-ac5d-9d2bb573184f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def test():  # 定义一个名为`test`的函数，用于测试模型的性能\n",
    "#     correct = 0  # 初始化正确预测的样本数量为0\n",
    "#     total = 0  # 初始化测试集中总样本数量为0\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():  # 设定PyTorch在此上下文中不计算梯度，这可以减小内存使用，加速计算，通常在评估模型时使用\n",
    "#         for data in test_loader:  # 遍历测试集中的每一批数据\n",
    "#             images, labels = data  # 将每一批数据分解为图像（输入）和标签（期望的输出）\n",
    "#             images = images.to(device)\n",
    "#             labels = labels.to(device)\n",
    "#             outputs = model(images)  # 用模型对每一批图像进行预测\n",
    "#             _, predicted = torch.max(outputs.data, dim=1)  # 从模型的输出中找出每个样本的最大值对应的类别，即预测的类别\n",
    "#                                                            # 返回的是一个元组，其中第一个元素是每行的最大值组成的张量，第二个元素是每行最大值对应的列索引组成的张量。\n",
    "#                                                            # max_values, max_indices = torch.max(predictions, dim=1)\n",
    "#             total += labels.size(0)  # 更新测试样本的总数\n",
    "#                                      # labels.size(0) 返回的是标签（labels）张量在第一个维度上的大小，也就是本批次（batch）中的样本数量。 ( N x c x w x h )\n",
    "#             correct += (predicted == labels).sum().item()  # 如果预测的类别与真实标签相等，就将正确预测的样本数加1\n",
    "#                                                            # 直接写 correct += (predicted == labels) 则会出错，因为这样 correct 会变成一个布尔值张量，\n",
    "#                                                            # 需要先用 .sum() 来求布尔张量中 True 的数量，然后用 .item() 转换为标量才能进行累加操作。\n",
    "#     print('accuracy on test set: %d %% ' % (100*correct/total))  # 计算并打印在测试集上的预测准确率\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f9bc4ef6-2014-480e-87b4-e93dabbfde5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#把测试封装成函数\n",
    "def test():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images,labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data,dim=1)       #从第一维度开始搜索\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted==labels).sum().item()\n",
    "\n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6e4a9dbd-05d5-4ff2-bc78-262ec7103d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.MNIST('./data/', train=False, download=True,\n",
    "                                transform=torchvision.transforms.Compose([\n",
    "                                torchvision.transforms.ToTensor(),\n",
    "                                torchvision.transforms.Normalize(\n",
    "                                (0.1307,), (0.3081,))\n",
    "                                ])),\n",
    "    batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c86edf-1bb2-49dc-bd5f-53944a1975f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:0 [0/60000 (0%)]\tLoss:0.018495\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    epochs=1\n",
    "    train(epochs)\n",
    "    # test()\n",
    "    print(test())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cdf395-47ca-4cb6-b8e7-8432e12052b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "03999fb3-41bd-444c-aca4-0641f1499ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9965\n"
     ]
    }
   ],
   "source": [
    "print(test())#$0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13090de4-f492-4729-a635-c7a9960e40f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9909\n"
     ]
    }
   ],
   "source": [
    "print(test())#$0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d5127ec-3d47-46e1-a162-a9028fce73b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9913\n"
     ]
    }
   ],
   "source": [
    "print(test())#$0.0015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1151d716-e07d-415d-9d13-9cd3eafb9d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型 state_dict()是一个字典，保存了网络中所有的参数\n",
    "# 转换并保存为torch.jit的模型\n",
    "example_input = torch.rand(1, 1, 28, 28).to(device)  # 创建一个形状为 (1, 1, 28, 28) 的随机输入样本，并将其放置在指定的设备上（例如 GPU）\n",
    "traced_model = torch.jit.trace(model, example_input)  # 使用 torch.jit.trace 函数对模型进行跟踪（tracing），生成跟踪模型（traced model）\n",
    "torch.jit.save(traced_model, \"traced_model_15.pt\")  # 将跟踪模型保存到 \"traced_model.pt\" 文件中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe4e42ff-9c21-410a-8e7a-85c7ebcac678",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (3793831218.py, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[17], line 7\u001b[1;36m\u001b[0m\n\u001b[1;33m    \"\"\"\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "# 保存模型 state_dict()是一个字典，保存了网络中所有的参数\n",
    "# 转换并保存为torch.jit的模型\n",
    "example_input = torch.rand(1, 1, 28, 28).to(device)  # 创建一个形状为 (1, 1, 28, 28) 的随机输入样本，并将其放置在指定的设备上（例如 GPU）\n",
    "traced_model = torch.jit.trace(model, example_input)  # 使用 torch.jit.trace 函数对模型进行跟踪（tracing），生成跟踪模型（traced model）\n",
    "torch.jit.save(traced_model, \"traced_model_13.pt\")  # 将跟踪模型保存到 \"traced_model.pt\" 文件中\n",
    "\n",
    "\"\"\"\n",
    "# 残差模块\n",
    "class ResidualBlock(torch.nn.Module):\n",
    "    #继承自ResidualBlock，传入输出通道数，输出通道数，滑动卷积核的步长\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "    #采用3x3卷积核进行卷积，同时注意进行填充以保持图像尺寸不变\n",
    "    self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
    "                            stride=stride, padding=1, bias=False)\n",
    "   \n",
    "    #激活函数采用RELU\n",
    "    self.relu = nn.ReLU(可选参数)\n",
    "   \n",
    "    #正向传播\n",
    "    def forward(self,x):\n",
    "        # 保存输入数据，采用恒等映射\n",
    "        identity = x\n",
    "        \n",
    "        #进行卷积和激活\n",
    "        out =self.conv1(x)\n",
    "        \n",
    "        out =self.relu(out)\n",
    "\n",
    "        \n",
    "        # 还原结果\n",
    "        删去了一句\n",
    "        \n",
    "        out = self.relu(out)\n",
    "        # 返回结果\n",
    "        return out\n",
    "        \n",
    "# 构建包含ResidualBlock的CNN\n",
    "class ResNet_CNN(nn.Module):\n",
    "    def __init__(self,num_classes=10):\n",
    "        super(ResNet_CNN,self).__init__()\n",
    "        \n",
    "        self.conv1 =nn.Conv2d(自行设计参数)\n",
    "        \n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.res2 = ResidualBlock(in_channels, out_channels, stride=1)\n",
    "        \n",
    "        # 用一个自适应均值池化层将每个通道维度变成1*1，此句可选\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        \n",
    "        self.fc = nn.Linear(需要自己根据网络调整参数)\n",
    "            \n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        \n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.res2(x)\n",
    "        \n",
    "        # n个通道，每个通道1*1，输出n*1*1\n",
    "        x = self.avg_pool(x)\n",
    "        # 将数据拉成一维\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9bb41b-1b18-4fba-9b25-23902896c00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初代\n",
    "# 构建包含ResidualBlock的CNN\n",
    "class ResNet_CNN(nn.Module):\n",
    "    def __init__(self,num_classes=10):\n",
    "        super(ResNet_CNN,self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.res1 = ResidualBlock(16, 16, stride=1)\n",
    "        self.res2 = ResidualBlock(32, 32, stride=1)\n",
    "        \n",
    "        self.mp = nn.MaxPool2d(2)\n",
    "        \n",
    "        # 用一个自适应均值池化层将每个通道维度变成1*1，此句可选\n",
    "        #self.avg_pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        \n",
    "        self.fc = nn.Linear(512, 10)\n",
    "        # self.fc = nn.Linear(num_channels * height * width, num_classes)\n",
    "        # MINIST数据集中的图像，通常的高度和宽度是28x28像素。\n",
    "    def forward(self,x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.mp(x)\n",
    "        x = self.res1(x)\n",
    "        \n",
    "        # n个通道，每个通道1*1，输出n*1*1\n",
    "        #x = self.avg_pool(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.mp(x)\n",
    "        x = self.res2(x)\n",
    "        \n",
    "\n",
    "        # 将数据拉成一维\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48630461-ed63-4631-9590-799305e65bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 78\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)  # 批归一化层\n",
    "        self.relu = nn.ReLU(inplace=True)  # ReLU激活函数\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)  # 批归一化层\n",
    "        self.stride = stride\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.bn1(self.conv1(x)))  # 第一个卷积层 + 批归一化 + ReLU激活函数\n",
    "        out = self.bn2(self.conv2(out))  # 第二个卷积层 + 批归一化\n",
    "        out += self.shortcut(x)  # 残差连接\n",
    "        out = self.relu(out)  # ReLU激活函数\n",
    "        return out\n",
    "\n",
    "class ResNet_CNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ResNet_CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=5)  # 输入通道数为1，输出通道数为16的卷积层\n",
    "        self.bn1 = nn.BatchNorm2d(16)  # 批归一化层\n",
    "        self.relu = nn.ReLU()  # ReLU激活函数\n",
    "        self.mp = nn.MaxPool2d(2)  # 最大池化层\n",
    "        self.res1 = ResidualBlock(16, 16, stride=1)  # 第一个残差块\n",
    "\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5)  # 输入通道数为16，输出通道数为32的卷积层\n",
    "        self.bn2 = nn.BatchNorm2d(32)  # 批归一化层\n",
    "        self.res2 = ResidualBlock(32, 32, stride=1)  # 第二个残差块\n",
    "\n",
    "        self.fc = nn.Linear(512, num_classes)  # 全连接层，将特征展平后连接到输出类别数的神经元\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "        out = self.conv1(x)  # 第一个卷积层\n",
    "        out = self.bn1(out)  # 批归一化层\n",
    "        out = self.relu(out)  # ReLU激活函数\n",
    "        out = self.mp(out)  # 最大池化层\n",
    "        out = self.res1(out)  # 第一个残差块\n",
    "        print(x.shape)\n",
    "        out = self.conv2(out)  # 第二个卷积层\n",
    "        out = self.bn2(out)  # 批归一化层\n",
    "        out = self.relu(out)  # ReLU激活函数\n",
    "        out = self.mp(out)  # 最大池化层\n",
    "        out = self.res2(out)  # 第二个残差块\n",
    "        print(x.shape)\n",
    "        out = out.view(out.size(0), -1)  # 将特征展平\n",
    "        out = self.fc(out)  # 全连接层\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c6aa20-5cc7-4a19-9111-e44e6c0219df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初代\n",
    "# 构建包含ResidualBlock的CNN\n",
    "class ResNet_CNN(nn.Module):\n",
    "    def __init__(self,num_classes=10):\n",
    "        super(ResNet_CNN,self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=5)\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=5)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.res1 = ResidualBlock(8, 8, stride=1)\n",
    "        self.res2 = ResidualBlock(16, 16, stride=1)\n",
    "        self.res3 = ResidualBlock(32, 32, stride=1)\n",
    "        \n",
    "        self.mp = nn.MaxPool2d(2)\n",
    "        \n",
    "        # 用一个自适应均值池化层将每个通道维度变成1*1，此句可选\n",
    "        #self.avg_pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        \n",
    "        self.fc = nn.Linear(512, 10)\n",
    "        # self.fc = nn.Linear(num_channels * height * width, num_classes)\n",
    "        # MINIST数据集中的图像，通常的高度和宽度是28x28像素。\n",
    "    def forward(self,x):\n",
    "        # 32x1x28x28\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.mp(x)\n",
    "        x = self.res1(x)\n",
    "        # 32x16x13x13\n",
    "        # n个通道，每个通道1*1，输出n*1*1\n",
    "        #x = self.avg_pool(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        #x = self.mp(x)\n",
    "        x = self.res2(x)\n",
    "        # 32x64x5x5\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        #x = self.mp(x)\n",
    "        x = self.res3(x)\n",
    "        # 32x32x5x5\n",
    "        # 将数据拉成一维\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834529e6-93d4-4912-8657-15748ad15409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 残差模块\n",
    "class ResidualBlock(torch.nn.Module):\n",
    "    #继承自ResidualBlock，传入输出通道数，输出通道数，滑动卷积核的步长\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        #采用3x3卷积核进行卷积，同时注意进行填充以保持图像尺寸不变\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
    "                                stride=stride, padding=1, bias=False)\n",
    "        #激活函数采用RELU\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    #正向传播，覆写torch.nn.Module中的魔法方法\n",
    "    def forward(self,x):\n",
    "        # 保存输入数据，采用恒等映射\n",
    "        identity = x\n",
    "        \n",
    "        #进行卷积和激活\n",
    "        out =self.conv1(x)\n",
    "        \n",
    "        out =self.relu(out)\n",
    "\n",
    "        \n",
    "        # 还原结果\n",
    "        out = out + x\n",
    "        \n",
    "        out = self.relu(out)\n",
    "        # 返回结果\n",
    "        return out\n",
    "# 初代\n",
    "# 构建包含ResidualBlock的CNN\n",
    "class ResNet_CNN(nn.Module):\n",
    "    def __init__(self,num_classes=10):\n",
    "        super(ResNet_CNN,self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=5， paddle=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, paddle=1)\n",
    "        self.conv3 = nn.Conv2d(channels, channels, kernel_size=1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.res1 = ResidualBlock(16, 16, stride=1)\n",
    "        self.res2 = ResidualBlock(32, 32, stride=1)\n",
    "        \n",
    "        self.mp = nn.MaxPool2d(2)\n",
    "        \n",
    "        # 用一个自适应均值池化层将每个通道维度变成1*1，此句可选\n",
    "        #self.avg_pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        \n",
    "        self.fc = nn.Linear(512, 10)\n",
    "        # self.fc = nn.Linear(num_channels * height * width, num_classes)\n",
    "        # MINIST数据集中的图像，通常的高度和宽度是28x28像素。\n",
    "    def forward(self,x):\n",
    "        in_size = x.size(0)\n",
    " \n",
    "        x = self.mp(F.relu(self.conv1(x)))\n",
    "        x = self.res1(x)\n",
    "        x = self.mp(F.relu(self.conv2(x)))\n",
    "        x = self.res2(x)\n",
    " \n",
    "        x = x.view(in_size, -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76320fec-855b-4e32-9d37-e7423f6a350d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:0 [0/60000 (0%)]\tLoss:2.313058\n",
      "Train Epoch:0 [640/60000 (1%)]\tLoss:1.880683\n",
      "Train Epoch:0 [1280/60000 (2%)]\tLoss:1.147544\n",
      "Train Epoch:0 [1920/60000 (3%)]\tLoss:0.966166\n",
      "Train Epoch:0 [2560/60000 (4%)]\tLoss:0.925112\n",
      "Train Epoch:0 [3200/60000 (5%)]\tLoss:0.389216\n",
      "Train Epoch:0 [3840/60000 (6%)]\tLoss:0.627616\n",
      "Train Epoch:0 [4480/60000 (7%)]\tLoss:0.266529\n",
      "Train Epoch:0 [5120/60000 (9%)]\tLoss:0.128691\n",
      "Train Epoch:0 [5760/60000 (10%)]\tLoss:0.108642\n",
      "Train Epoch:0 [6400/60000 (11%)]\tLoss:0.214485\n",
      "Train Epoch:0 [7040/60000 (12%)]\tLoss:0.287485\n",
      "Train Epoch:0 [7680/60000 (13%)]\tLoss:0.125609\n",
      "Train Epoch:0 [8320/60000 (14%)]\tLoss:0.120873\n",
      "Train Epoch:0 [8960/60000 (15%)]\tLoss:0.280016\n",
      "Train Epoch:0 [9600/60000 (16%)]\tLoss:0.266129\n",
      "Train Epoch:0 [10240/60000 (17%)]\tLoss:0.132808\n",
      "Train Epoch:0 [10880/60000 (18%)]\tLoss:0.146302\n",
      "Train Epoch:0 [11520/60000 (19%)]\tLoss:0.200040\n",
      "Train Epoch:0 [12160/60000 (20%)]\tLoss:0.090015\n",
      "Train Epoch:0 [12800/60000 (21%)]\tLoss:0.139538\n",
      "Train Epoch:0 [13440/60000 (22%)]\tLoss:0.128670\n",
      "Train Epoch:0 [14080/60000 (23%)]\tLoss:0.173283\n",
      "Train Epoch:0 [14720/60000 (25%)]\tLoss:0.074368\n",
      "Train Epoch:0 [15360/60000 (26%)]\tLoss:0.095382\n",
      "Train Epoch:0 [16000/60000 (27%)]\tLoss:0.193957\n",
      "Train Epoch:0 [16640/60000 (28%)]\tLoss:0.072326\n",
      "Train Epoch:0 [17280/60000 (29%)]\tLoss:0.082860\n",
      "Train Epoch:0 [17920/60000 (30%)]\tLoss:0.220738\n",
      "Train Epoch:0 [18560/60000 (31%)]\tLoss:0.075102\n",
      "Train Epoch:0 [19200/60000 (32%)]\tLoss:0.151478\n",
      "Train Epoch:0 [19840/60000 (33%)]\tLoss:0.193067\n",
      "Train Epoch:0 [20480/60000 (34%)]\tLoss:0.248652\n",
      "Train Epoch:0 [21120/60000 (35%)]\tLoss:0.073913\n",
      "Train Epoch:0 [21760/60000 (36%)]\tLoss:0.147855\n",
      "Train Epoch:0 [22400/60000 (37%)]\tLoss:0.100887\n",
      "Train Epoch:0 [23040/60000 (38%)]\tLoss:0.191996\n",
      "Train Epoch:0 [23680/60000 (39%)]\tLoss:0.329124\n",
      "Train Epoch:0 [24320/60000 (41%)]\tLoss:0.074158\n",
      "Train Epoch:0 [24960/60000 (42%)]\tLoss:0.080437\n",
      "Train Epoch:0 [25600/60000 (43%)]\tLoss:0.070093\n",
      "Train Epoch:0 [26240/60000 (44%)]\tLoss:0.075189\n",
      "Train Epoch:0 [26880/60000 (45%)]\tLoss:0.278341\n",
      "Train Epoch:0 [27520/60000 (46%)]\tLoss:0.112362\n",
      "Train Epoch:0 [28160/60000 (47%)]\tLoss:0.124257\n",
      "Train Epoch:0 [28800/60000 (48%)]\tLoss:0.113684\n",
      "Train Epoch:0 [29440/60000 (49%)]\tLoss:0.184672\n",
      "Train Epoch:0 [30080/60000 (50%)]\tLoss:0.281413\n",
      "Train Epoch:0 [30720/60000 (51%)]\tLoss:0.165737\n",
      "Train Epoch:0 [31360/60000 (52%)]\tLoss:0.096374\n",
      "Train Epoch:0 [32000/60000 (53%)]\tLoss:0.067197\n",
      "Train Epoch:0 [32640/60000 (54%)]\tLoss:0.047041\n",
      "Train Epoch:0 [33280/60000 (55%)]\tLoss:0.021131\n",
      "Train Epoch:0 [33920/60000 (57%)]\tLoss:0.096426\n",
      "Train Epoch:0 [34560/60000 (58%)]\tLoss:0.215090\n",
      "Train Epoch:0 [35200/60000 (59%)]\tLoss:0.057887\n",
      "Train Epoch:0 [35840/60000 (60%)]\tLoss:0.154436\n",
      "Train Epoch:0 [36480/60000 (61%)]\tLoss:0.086391\n",
      "Train Epoch:0 [37120/60000 (62%)]\tLoss:0.191410\n",
      "Train Epoch:0 [37760/60000 (63%)]\tLoss:0.113138\n",
      "Train Epoch:0 [38400/60000 (64%)]\tLoss:0.203606\n",
      "Train Epoch:0 [39040/60000 (65%)]\tLoss:0.092349\n",
      "Train Epoch:0 [39680/60000 (66%)]\tLoss:0.119401\n",
      "Train Epoch:0 [40320/60000 (67%)]\tLoss:0.244736\n",
      "Train Epoch:0 [40960/60000 (68%)]\tLoss:0.117674\n",
      "Train Epoch:0 [41600/60000 (69%)]\tLoss:0.062476\n",
      "Train Epoch:0 [42240/60000 (70%)]\tLoss:0.127338\n",
      "Train Epoch:0 [42880/60000 (71%)]\tLoss:0.086022\n",
      "Train Epoch:0 [43520/60000 (72%)]\tLoss:0.072694\n",
      "Train Epoch:0 [44160/60000 (74%)]\tLoss:0.068864\n",
      "Train Epoch:0 [44800/60000 (75%)]\tLoss:0.252830\n",
      "Train Epoch:0 [45440/60000 (76%)]\tLoss:0.086065\n",
      "Train Epoch:0 [46080/60000 (77%)]\tLoss:0.148520\n",
      "Train Epoch:0 [46720/60000 (78%)]\tLoss:0.004034\n",
      "Train Epoch:0 [47360/60000 (79%)]\tLoss:0.155028\n",
      "Train Epoch:0 [48000/60000 (80%)]\tLoss:0.140869\n",
      "Train Epoch:0 [48640/60000 (81%)]\tLoss:0.114370\n",
      "Train Epoch:0 [49280/60000 (82%)]\tLoss:0.072334\n",
      "Train Epoch:0 [49920/60000 (83%)]\tLoss:0.023936\n",
      "Train Epoch:0 [50560/60000 (84%)]\tLoss:0.082812\n",
      "Train Epoch:0 [51200/60000 (85%)]\tLoss:0.049726\n",
      "Train Epoch:0 [51840/60000 (86%)]\tLoss:0.084654\n",
      "Train Epoch:0 [52480/60000 (87%)]\tLoss:0.092858\n",
      "Train Epoch:0 [53120/60000 (88%)]\tLoss:0.057467\n",
      "Train Epoch:0 [53760/60000 (90%)]\tLoss:0.016175\n",
      "Train Epoch:0 [54400/60000 (91%)]\tLoss:0.067798\n",
      "Train Epoch:0 [55040/60000 (92%)]\tLoss:0.139549\n",
      "Train Epoch:0 [55680/60000 (93%)]\tLoss:0.047091\n",
      "Train Epoch:0 [56320/60000 (94%)]\tLoss:0.061747\n",
      "Train Epoch:0 [56960/60000 (95%)]\tLoss:0.059086\n",
      "Train Epoch:0 [57600/60000 (96%)]\tLoss:0.081238\n",
      "Train Epoch:0 [58240/60000 (97%)]\tLoss:0.098094\n",
      "Train Epoch:0 [58880/60000 (98%)]\tLoss:0.026499\n",
      "Train Epoch:0 [59520/60000 (99%)]\tLoss:0.217438\n",
      "Train Epoch:1 [0/60000 (0%)]\tLoss:0.065796\n",
      "Train Epoch:1 [640/60000 (1%)]\tLoss:0.151189\n",
      "Train Epoch:1 [1280/60000 (2%)]\tLoss:0.082411\n",
      "Train Epoch:1 [1920/60000 (3%)]\tLoss:0.094497\n",
      "Train Epoch:1 [2560/60000 (4%)]\tLoss:0.082855\n",
      "Train Epoch:1 [3200/60000 (5%)]\tLoss:0.017197\n",
      "Train Epoch:1 [3840/60000 (6%)]\tLoss:0.055786\n",
      "Train Epoch:1 [4480/60000 (7%)]\tLoss:0.007730\n",
      "Train Epoch:1 [5120/60000 (9%)]\tLoss:0.002491\n",
      "Train Epoch:1 [5760/60000 (10%)]\tLoss:0.051234\n",
      "Train Epoch:1 [6400/60000 (11%)]\tLoss:0.031798\n",
      "Train Epoch:1 [7040/60000 (12%)]\tLoss:0.018600\n",
      "Train Epoch:1 [7680/60000 (13%)]\tLoss:0.029043\n",
      "Train Epoch:1 [8320/60000 (14%)]\tLoss:0.014988\n",
      "Train Epoch:1 [8960/60000 (15%)]\tLoss:0.016113\n",
      "Train Epoch:1 [9600/60000 (16%)]\tLoss:0.012263\n",
      "Train Epoch:1 [10240/60000 (17%)]\tLoss:0.111445\n",
      "Train Epoch:1 [10880/60000 (18%)]\tLoss:0.189283\n",
      "Train Epoch:1 [11520/60000 (19%)]\tLoss:0.063857\n",
      "Train Epoch:1 [12160/60000 (20%)]\tLoss:0.014932\n",
      "Train Epoch:1 [12800/60000 (21%)]\tLoss:0.042559\n",
      "Train Epoch:1 [13440/60000 (22%)]\tLoss:0.018260\n",
      "Train Epoch:1 [14080/60000 (23%)]\tLoss:0.128059\n",
      "Train Epoch:1 [14720/60000 (25%)]\tLoss:0.004678\n",
      "Train Epoch:1 [15360/60000 (26%)]\tLoss:0.082347\n",
      "Train Epoch:1 [16000/60000 (27%)]\tLoss:0.028281\n",
      "Train Epoch:1 [16640/60000 (28%)]\tLoss:0.061892\n",
      "Train Epoch:1 [17280/60000 (29%)]\tLoss:0.008238\n",
      "Train Epoch:1 [17920/60000 (30%)]\tLoss:0.118093\n",
      "Train Epoch:1 [18560/60000 (31%)]\tLoss:0.046768\n",
      "Train Epoch:1 [19200/60000 (32%)]\tLoss:0.124597\n",
      "Train Epoch:1 [19840/60000 (33%)]\tLoss:0.073853\n",
      "Train Epoch:1 [20480/60000 (34%)]\tLoss:0.058762\n",
      "Train Epoch:1 [21120/60000 (35%)]\tLoss:0.134213\n",
      "Train Epoch:1 [21760/60000 (36%)]\tLoss:0.005955\n",
      "Train Epoch:1 [22400/60000 (37%)]\tLoss:0.021998\n",
      "Train Epoch:1 [23040/60000 (38%)]\tLoss:0.040761\n",
      "Train Epoch:1 [23680/60000 (39%)]\tLoss:0.016918\n",
      "Train Epoch:1 [24320/60000 (41%)]\tLoss:0.057786\n",
      "Train Epoch:1 [24960/60000 (42%)]\tLoss:0.017760\n",
      "Train Epoch:1 [25600/60000 (43%)]\tLoss:0.400060\n",
      "Train Epoch:1 [26240/60000 (44%)]\tLoss:0.024603\n",
      "Train Epoch:1 [26880/60000 (45%)]\tLoss:0.107456\n",
      "Train Epoch:1 [27520/60000 (46%)]\tLoss:0.037191\n",
      "Train Epoch:1 [28160/60000 (47%)]\tLoss:0.003941\n",
      "Train Epoch:1 [28800/60000 (48%)]\tLoss:0.021900\n",
      "Train Epoch:1 [29440/60000 (49%)]\tLoss:0.009782\n",
      "Train Epoch:1 [30080/60000 (50%)]\tLoss:0.021776\n",
      "Train Epoch:1 [30720/60000 (51%)]\tLoss:0.020845\n",
      "Train Epoch:1 [31360/60000 (52%)]\tLoss:0.012999\n",
      "Train Epoch:1 [32000/60000 (53%)]\tLoss:0.049365\n",
      "Train Epoch:1 [32640/60000 (54%)]\tLoss:0.058460\n",
      "Train Epoch:1 [33280/60000 (55%)]\tLoss:0.012808\n",
      "Train Epoch:1 [33920/60000 (57%)]\tLoss:0.035920\n",
      "Train Epoch:1 [34560/60000 (58%)]\tLoss:0.154823\n",
      "Train Epoch:1 [35200/60000 (59%)]\tLoss:0.041712\n",
      "Train Epoch:1 [35840/60000 (60%)]\tLoss:0.025357\n",
      "Train Epoch:1 [36480/60000 (61%)]\tLoss:0.016587\n",
      "Train Epoch:1 [37120/60000 (62%)]\tLoss:0.040949\n",
      "Train Epoch:1 [37760/60000 (63%)]\tLoss:0.067320\n",
      "Train Epoch:1 [38400/60000 (64%)]\tLoss:0.093283\n",
      "Train Epoch:1 [39040/60000 (65%)]\tLoss:0.118900\n",
      "Train Epoch:1 [39680/60000 (66%)]\tLoss:0.039005\n",
      "Train Epoch:1 [40320/60000 (67%)]\tLoss:0.004751\n",
      "Train Epoch:1 [40960/60000 (68%)]\tLoss:0.149377\n",
      "Train Epoch:1 [41600/60000 (69%)]\tLoss:0.171842\n",
      "Train Epoch:1 [42240/60000 (70%)]\tLoss:0.056454\n",
      "Train Epoch:1 [42880/60000 (71%)]\tLoss:0.022153\n",
      "Train Epoch:1 [43520/60000 (72%)]\tLoss:0.031178\n",
      "Train Epoch:1 [44160/60000 (74%)]\tLoss:0.004597\n",
      "Train Epoch:1 [44800/60000 (75%)]\tLoss:0.296490\n",
      "Train Epoch:1 [45440/60000 (76%)]\tLoss:0.102338\n",
      "Train Epoch:1 [46080/60000 (77%)]\tLoss:0.059285\n",
      "Train Epoch:1 [46720/60000 (78%)]\tLoss:0.021514\n",
      "Train Epoch:1 [47360/60000 (79%)]\tLoss:0.371022\n",
      "Train Epoch:1 [48000/60000 (80%)]\tLoss:0.038518\n",
      "Train Epoch:1 [48640/60000 (81%)]\tLoss:0.003229\n",
      "Train Epoch:1 [49280/60000 (82%)]\tLoss:0.034142\n",
      "Train Epoch:1 [49920/60000 (83%)]\tLoss:0.126888\n",
      "Train Epoch:1 [50560/60000 (84%)]\tLoss:0.236698\n",
      "Train Epoch:1 [51200/60000 (85%)]\tLoss:0.086593\n",
      "Train Epoch:1 [51840/60000 (86%)]\tLoss:0.012807\n",
      "Train Epoch:1 [52480/60000 (87%)]\tLoss:0.050017\n",
      "Train Epoch:1 [53120/60000 (88%)]\tLoss:0.091467\n",
      "Train Epoch:1 [53760/60000 (90%)]\tLoss:0.074666\n",
      "Train Epoch:1 [54400/60000 (91%)]\tLoss:0.015041\n",
      "Train Epoch:1 [55040/60000 (92%)]\tLoss:0.012582\n",
      "Train Epoch:1 [55680/60000 (93%)]\tLoss:0.010369\n",
      "Train Epoch:1 [56320/60000 (94%)]\tLoss:0.001667\n",
      "Train Epoch:1 [56960/60000 (95%)]\tLoss:0.075842\n",
      "Train Epoch:1 [57600/60000 (96%)]\tLoss:0.079515\n",
      "Train Epoch:1 [58240/60000 (97%)]\tLoss:0.018130\n",
      "Train Epoch:1 [58880/60000 (98%)]\tLoss:0.106997\n",
      "Train Epoch:1 [59520/60000 (99%)]\tLoss:0.012031\n",
      "Train Epoch:2 [0/60000 (0%)]\tLoss:0.004469\n",
      "Train Epoch:2 [640/60000 (1%)]\tLoss:0.053277\n",
      "Train Epoch:2 [1280/60000 (2%)]\tLoss:0.128299\n",
      "Train Epoch:2 [1920/60000 (3%)]\tLoss:0.002886\n",
      "Train Epoch:2 [2560/60000 (4%)]\tLoss:0.041972\n",
      "Train Epoch:2 [3200/60000 (5%)]\tLoss:0.037897\n",
      "Train Epoch:2 [3840/60000 (6%)]\tLoss:0.023932\n",
      "Train Epoch:2 [4480/60000 (7%)]\tLoss:0.003253\n",
      "Train Epoch:2 [5120/60000 (9%)]\tLoss:0.062356\n",
      "Train Epoch:2 [5760/60000 (10%)]\tLoss:0.014914\n",
      "Train Epoch:2 [6400/60000 (11%)]\tLoss:0.017470\n",
      "Train Epoch:2 [7040/60000 (12%)]\tLoss:0.047682\n",
      "Train Epoch:2 [7680/60000 (13%)]\tLoss:0.003225\n",
      "Train Epoch:2 [8320/60000 (14%)]\tLoss:0.020203\n",
      "Train Epoch:2 [8960/60000 (15%)]\tLoss:0.018733\n",
      "Train Epoch:2 [9600/60000 (16%)]\tLoss:0.020604\n",
      "Train Epoch:2 [10240/60000 (17%)]\tLoss:0.011424\n",
      "Train Epoch:2 [10880/60000 (18%)]\tLoss:0.001698\n",
      "Train Epoch:2 [11520/60000 (19%)]\tLoss:0.011319\n",
      "Train Epoch:2 [12160/60000 (20%)]\tLoss:0.037337\n",
      "Train Epoch:2 [12800/60000 (21%)]\tLoss:0.041689\n",
      "Train Epoch:2 [13440/60000 (22%)]\tLoss:0.066911\n",
      "Train Epoch:2 [14080/60000 (23%)]\tLoss:0.165073\n",
      "Train Epoch:2 [14720/60000 (25%)]\tLoss:0.012809\n",
      "Train Epoch:2 [15360/60000 (26%)]\tLoss:0.007638\n",
      "Train Epoch:2 [16000/60000 (27%)]\tLoss:0.003522\n",
      "Train Epoch:2 [16640/60000 (28%)]\tLoss:0.000902\n",
      "Train Epoch:2 [17280/60000 (29%)]\tLoss:0.053390\n",
      "Train Epoch:2 [17920/60000 (30%)]\tLoss:0.032977\n",
      "Train Epoch:2 [18560/60000 (31%)]\tLoss:0.001124\n",
      "Train Epoch:2 [19200/60000 (32%)]\tLoss:0.028888\n",
      "Train Epoch:2 [19840/60000 (33%)]\tLoss:0.060485\n",
      "Train Epoch:2 [20480/60000 (34%)]\tLoss:0.093608\n",
      "Train Epoch:2 [21120/60000 (35%)]\tLoss:0.002621\n",
      "Train Epoch:2 [21760/60000 (36%)]\tLoss:0.055541\n",
      "Train Epoch:2 [22400/60000 (37%)]\tLoss:0.257101\n",
      "Train Epoch:2 [23040/60000 (38%)]\tLoss:0.123613\n",
      "Train Epoch:2 [23680/60000 (39%)]\tLoss:0.045314\n",
      "Train Epoch:2 [24320/60000 (41%)]\tLoss:0.075593\n",
      "Train Epoch:2 [24960/60000 (42%)]\tLoss:0.004924\n",
      "Train Epoch:2 [25600/60000 (43%)]\tLoss:0.013667\n",
      "Train Epoch:2 [26240/60000 (44%)]\tLoss:0.006042\n",
      "Train Epoch:2 [26880/60000 (45%)]\tLoss:0.037144\n",
      "Train Epoch:2 [27520/60000 (46%)]\tLoss:0.032095\n",
      "Train Epoch:2 [28160/60000 (47%)]\tLoss:0.002442\n",
      "Train Epoch:2 [28800/60000 (48%)]\tLoss:0.030029\n",
      "Train Epoch:2 [29440/60000 (49%)]\tLoss:0.010995\n",
      "Train Epoch:2 [30080/60000 (50%)]\tLoss:0.011437\n",
      "Train Epoch:2 [30720/60000 (51%)]\tLoss:0.007116\n",
      "Train Epoch:2 [31360/60000 (52%)]\tLoss:0.050958\n",
      "Train Epoch:2 [32000/60000 (53%)]\tLoss:0.025270\n",
      "Train Epoch:2 [32640/60000 (54%)]\tLoss:0.035729\n",
      "Train Epoch:2 [33280/60000 (55%)]\tLoss:0.158398\n",
      "Train Epoch:2 [33920/60000 (57%)]\tLoss:0.006955\n",
      "Train Epoch:2 [34560/60000 (58%)]\tLoss:0.001651\n",
      "Train Epoch:2 [35200/60000 (59%)]\tLoss:0.023021\n",
      "Train Epoch:2 [35840/60000 (60%)]\tLoss:0.114759\n",
      "Train Epoch:2 [36480/60000 (61%)]\tLoss:0.027064\n",
      "Train Epoch:2 [37120/60000 (62%)]\tLoss:0.037113\n",
      "Train Epoch:2 [37760/60000 (63%)]\tLoss:0.008585\n",
      "Train Epoch:2 [38400/60000 (64%)]\tLoss:0.028579\n",
      "Train Epoch:2 [39040/60000 (65%)]\tLoss:0.082265\n",
      "Train Epoch:2 [39680/60000 (66%)]\tLoss:0.009051\n",
      "Train Epoch:2 [40320/60000 (67%)]\tLoss:0.014030\n",
      "Train Epoch:2 [40960/60000 (68%)]\tLoss:0.016797\n",
      "Train Epoch:2 [41600/60000 (69%)]\tLoss:0.103339\n",
      "Train Epoch:2 [42240/60000 (70%)]\tLoss:0.085934\n",
      "Train Epoch:2 [42880/60000 (71%)]\tLoss:0.002686\n",
      "Train Epoch:2 [43520/60000 (72%)]\tLoss:0.022000\n",
      "Train Epoch:2 [44160/60000 (74%)]\tLoss:0.037699\n",
      "Train Epoch:2 [44800/60000 (75%)]\tLoss:0.004470\n",
      "Train Epoch:2 [45440/60000 (76%)]\tLoss:0.008315\n",
      "Train Epoch:2 [46080/60000 (77%)]\tLoss:0.023441\n",
      "Train Epoch:2 [46720/60000 (78%)]\tLoss:0.011280\n",
      "Train Epoch:2 [47360/60000 (79%)]\tLoss:0.111742\n",
      "Train Epoch:2 [48000/60000 (80%)]\tLoss:0.048725\n",
      "Train Epoch:2 [48640/60000 (81%)]\tLoss:0.119399\n",
      "Train Epoch:2 [49280/60000 (82%)]\tLoss:0.002939\n",
      "Train Epoch:2 [49920/60000 (83%)]\tLoss:0.010982\n",
      "Train Epoch:2 [50560/60000 (84%)]\tLoss:0.132178\n",
      "Train Epoch:2 [51200/60000 (85%)]\tLoss:0.051042\n",
      "Train Epoch:2 [51840/60000 (86%)]\tLoss:0.066963\n",
      "Train Epoch:2 [52480/60000 (87%)]\tLoss:0.008237\n",
      "Train Epoch:2 [53120/60000 (88%)]\tLoss:0.173966\n",
      "Train Epoch:2 [53760/60000 (90%)]\tLoss:0.005984\n",
      "Train Epoch:2 [54400/60000 (91%)]\tLoss:0.083261\n",
      "Train Epoch:2 [55040/60000 (92%)]\tLoss:0.019061\n",
      "Train Epoch:2 [55680/60000 (93%)]\tLoss:0.010139\n",
      "Train Epoch:2 [56320/60000 (94%)]\tLoss:0.007017\n",
      "Train Epoch:2 [56960/60000 (95%)]\tLoss:0.002466\n",
      "Train Epoch:2 [57600/60000 (96%)]\tLoss:0.182521\n",
      "Train Epoch:2 [58240/60000 (97%)]\tLoss:0.013451\n",
      "Train Epoch:2 [58880/60000 (98%)]\tLoss:0.240732\n",
      "Train Epoch:2 [59520/60000 (99%)]\tLoss:0.100780\n",
      "Train Epoch:3 [0/60000 (0%)]\tLoss:0.100509\n",
      "Train Epoch:3 [640/60000 (1%)]\tLoss:0.025543\n",
      "Train Epoch:3 [1280/60000 (2%)]\tLoss:0.094169\n",
      "Train Epoch:3 [1920/60000 (3%)]\tLoss:0.013297\n",
      "Train Epoch:3 [2560/60000 (4%)]\tLoss:0.009519\n",
      "Train Epoch:3 [3200/60000 (5%)]\tLoss:0.063076\n",
      "Train Epoch:3 [3840/60000 (6%)]\tLoss:0.060426\n",
      "Train Epoch:3 [4480/60000 (7%)]\tLoss:0.049238\n",
      "Train Epoch:3 [5120/60000 (9%)]\tLoss:0.003337\n",
      "Train Epoch:3 [5760/60000 (10%)]\tLoss:0.038865\n",
      "Train Epoch:3 [6400/60000 (11%)]\tLoss:0.032735\n",
      "Train Epoch:3 [7040/60000 (12%)]\tLoss:0.025660\n",
      "Train Epoch:3 [7680/60000 (13%)]\tLoss:0.006358\n",
      "Train Epoch:3 [8320/60000 (14%)]\tLoss:0.058849\n",
      "Train Epoch:3 [8960/60000 (15%)]\tLoss:0.006908\n",
      "Train Epoch:3 [9600/60000 (16%)]\tLoss:0.002467\n",
      "Train Epoch:3 [10240/60000 (17%)]\tLoss:0.046106\n",
      "Train Epoch:3 [10880/60000 (18%)]\tLoss:0.110040\n",
      "Train Epoch:3 [11520/60000 (19%)]\tLoss:0.030451\n",
      "Train Epoch:3 [12160/60000 (20%)]\tLoss:0.151377\n",
      "Train Epoch:3 [12800/60000 (21%)]\tLoss:0.004729\n",
      "Train Epoch:3 [13440/60000 (22%)]\tLoss:0.016136\n",
      "Train Epoch:3 [14080/60000 (23%)]\tLoss:0.014195\n",
      "Train Epoch:3 [14720/60000 (25%)]\tLoss:0.039994\n",
      "Train Epoch:3 [15360/60000 (26%)]\tLoss:0.008960\n",
      "Train Epoch:3 [16000/60000 (27%)]\tLoss:0.001540\n",
      "Train Epoch:3 [16640/60000 (28%)]\tLoss:0.130995\n",
      "Train Epoch:3 [17280/60000 (29%)]\tLoss:0.003779\n",
      "Train Epoch:3 [17920/60000 (30%)]\tLoss:0.039404\n",
      "Train Epoch:3 [18560/60000 (31%)]\tLoss:0.013318\n",
      "Train Epoch:3 [19200/60000 (32%)]\tLoss:0.003139\n",
      "Train Epoch:3 [19840/60000 (33%)]\tLoss:0.086562\n",
      "Train Epoch:3 [20480/60000 (34%)]\tLoss:0.041742\n",
      "Train Epoch:3 [21120/60000 (35%)]\tLoss:0.001120\n",
      "Train Epoch:3 [21760/60000 (36%)]\tLoss:0.047762\n",
      "Train Epoch:3 [22400/60000 (37%)]\tLoss:0.007374\n",
      "Train Epoch:3 [23040/60000 (38%)]\tLoss:0.007116\n",
      "Train Epoch:3 [23680/60000 (39%)]\tLoss:0.004195\n",
      "Train Epoch:3 [24320/60000 (41%)]\tLoss:0.058962\n",
      "Train Epoch:3 [24960/60000 (42%)]\tLoss:0.010530\n",
      "Train Epoch:3 [25600/60000 (43%)]\tLoss:0.001870\n",
      "Train Epoch:3 [26240/60000 (44%)]\tLoss:0.002202\n",
      "Train Epoch:3 [26880/60000 (45%)]\tLoss:0.013329\n",
      "Train Epoch:3 [27520/60000 (46%)]\tLoss:0.010700\n",
      "Train Epoch:3 [28160/60000 (47%)]\tLoss:0.040682\n",
      "Train Epoch:3 [28800/60000 (48%)]\tLoss:0.001373\n",
      "Train Epoch:3 [29440/60000 (49%)]\tLoss:0.031033\n",
      "Train Epoch:3 [30080/60000 (50%)]\tLoss:0.156391\n",
      "Train Epoch:3 [30720/60000 (51%)]\tLoss:0.116563\n",
      "Train Epoch:3 [31360/60000 (52%)]\tLoss:0.106460\n",
      "Train Epoch:3 [32000/60000 (53%)]\tLoss:0.031479\n",
      "Train Epoch:3 [32640/60000 (54%)]\tLoss:0.010721\n",
      "Train Epoch:3 [33280/60000 (55%)]\tLoss:0.075986\n",
      "Train Epoch:3 [33920/60000 (57%)]\tLoss:0.074493\n",
      "Train Epoch:3 [34560/60000 (58%)]\tLoss:0.044719\n",
      "Train Epoch:3 [35200/60000 (59%)]\tLoss:0.046900\n",
      "Train Epoch:3 [35840/60000 (60%)]\tLoss:0.027443\n",
      "Train Epoch:3 [36480/60000 (61%)]\tLoss:0.059128\n",
      "Train Epoch:3 [37120/60000 (62%)]\tLoss:0.101897\n",
      "Train Epoch:3 [37760/60000 (63%)]\tLoss:0.011265\n",
      "Train Epoch:3 [38400/60000 (64%)]\tLoss:0.165813\n",
      "Train Epoch:3 [39040/60000 (65%)]\tLoss:0.014991\n",
      "Train Epoch:3 [39680/60000 (66%)]\tLoss:0.050898\n",
      "Train Epoch:3 [40320/60000 (67%)]\tLoss:0.062217\n",
      "Train Epoch:3 [40960/60000 (68%)]\tLoss:0.088194\n",
      "Train Epoch:3 [41600/60000 (69%)]\tLoss:0.009946\n",
      "Train Epoch:3 [42240/60000 (70%)]\tLoss:0.002248\n",
      "Train Epoch:3 [42880/60000 (71%)]\tLoss:0.058863\n",
      "Train Epoch:3 [43520/60000 (72%)]\tLoss:0.151498\n",
      "Train Epoch:3 [44160/60000 (74%)]\tLoss:0.037535\n",
      "Train Epoch:3 [44800/60000 (75%)]\tLoss:0.001126\n",
      "Train Epoch:3 [45440/60000 (76%)]\tLoss:0.016716\n",
      "Train Epoch:3 [46080/60000 (77%)]\tLoss:0.006489\n",
      "Train Epoch:3 [46720/60000 (78%)]\tLoss:0.102995\n",
      "Train Epoch:3 [47360/60000 (79%)]\tLoss:0.004280\n",
      "Train Epoch:3 [48000/60000 (80%)]\tLoss:0.120353\n",
      "Train Epoch:3 [48640/60000 (81%)]\tLoss:0.001093\n",
      "Train Epoch:3 [49280/60000 (82%)]\tLoss:0.002027\n",
      "Train Epoch:3 [49920/60000 (83%)]\tLoss:0.000757\n",
      "Train Epoch:3 [50560/60000 (84%)]\tLoss:0.055905\n",
      "Train Epoch:3 [51200/60000 (85%)]\tLoss:0.027086\n",
      "Train Epoch:3 [51840/60000 (86%)]\tLoss:0.007709\n",
      "Train Epoch:3 [52480/60000 (87%)]\tLoss:0.002141\n",
      "Train Epoch:3 [53120/60000 (88%)]\tLoss:0.156014\n",
      "Train Epoch:3 [53760/60000 (90%)]\tLoss:0.068781\n",
      "Train Epoch:3 [54400/60000 (91%)]\tLoss:0.092207\n",
      "Train Epoch:3 [55040/60000 (92%)]\tLoss:0.008078\n",
      "Train Epoch:3 [55680/60000 (93%)]\tLoss:0.005952\n",
      "Train Epoch:3 [56320/60000 (94%)]\tLoss:0.008872\n",
      "Train Epoch:3 [56960/60000 (95%)]\tLoss:0.003708\n",
      "Train Epoch:3 [57600/60000 (96%)]\tLoss:0.034972\n",
      "Train Epoch:3 [58240/60000 (97%)]\tLoss:0.005510\n",
      "Train Epoch:3 [58880/60000 (98%)]\tLoss:0.039448\n",
      "Train Epoch:3 [59520/60000 (99%)]\tLoss:0.139745\n",
      "Train Epoch:4 [0/60000 (0%)]\tLoss:0.124886\n",
      "Train Epoch:4 [640/60000 (1%)]\tLoss:0.021445\n",
      "Train Epoch:4 [1280/60000 (2%)]\tLoss:0.014020\n",
      "Train Epoch:4 [1920/60000 (3%)]\tLoss:0.010710\n",
      "Train Epoch:4 [2560/60000 (4%)]\tLoss:0.002412\n",
      "Train Epoch:4 [3200/60000 (5%)]\tLoss:0.001854\n",
      "Train Epoch:4 [3840/60000 (6%)]\tLoss:0.014798\n",
      "Train Epoch:4 [4480/60000 (7%)]\tLoss:0.119020\n",
      "Train Epoch:4 [5120/60000 (9%)]\tLoss:0.127144\n",
      "Train Epoch:4 [5760/60000 (10%)]\tLoss:0.011680\n",
      "Train Epoch:4 [6400/60000 (11%)]\tLoss:0.080237\n",
      "Train Epoch:4 [7040/60000 (12%)]\tLoss:0.014633\n",
      "Train Epoch:4 [7680/60000 (13%)]\tLoss:0.014788\n",
      "Train Epoch:4 [8320/60000 (14%)]\tLoss:0.004130\n",
      "Train Epoch:4 [8960/60000 (15%)]\tLoss:0.062099\n",
      "Train Epoch:4 [9600/60000 (16%)]\tLoss:0.002154\n",
      "Train Epoch:4 [10240/60000 (17%)]\tLoss:0.006042\n",
      "Train Epoch:4 [10880/60000 (18%)]\tLoss:0.018887\n",
      "Train Epoch:4 [11520/60000 (19%)]\tLoss:0.029652\n",
      "Train Epoch:4 [12160/60000 (20%)]\tLoss:0.238335\n",
      "Train Epoch:4 [12800/60000 (21%)]\tLoss:0.001296\n",
      "Train Epoch:4 [13440/60000 (22%)]\tLoss:0.048517\n",
      "Train Epoch:4 [14080/60000 (23%)]\tLoss:0.008663\n",
      "Train Epoch:4 [14720/60000 (25%)]\tLoss:0.005607\n",
      "Train Epoch:4 [15360/60000 (26%)]\tLoss:0.006637\n",
      "Train Epoch:4 [16000/60000 (27%)]\tLoss:0.003429\n",
      "Train Epoch:4 [16640/60000 (28%)]\tLoss:0.001742\n",
      "Train Epoch:4 [17280/60000 (29%)]\tLoss:0.019548\n",
      "Train Epoch:4 [17920/60000 (30%)]\tLoss:0.000472\n",
      "Train Epoch:4 [18560/60000 (31%)]\tLoss:0.001381\n",
      "Train Epoch:4 [19200/60000 (32%)]\tLoss:0.000561\n",
      "Train Epoch:4 [19840/60000 (33%)]\tLoss:0.172489\n",
      "Train Epoch:4 [20480/60000 (34%)]\tLoss:0.002510\n",
      "Train Epoch:4 [21120/60000 (35%)]\tLoss:0.017830\n",
      "Train Epoch:4 [21760/60000 (36%)]\tLoss:0.036394\n",
      "Train Epoch:4 [22400/60000 (37%)]\tLoss:0.084681\n",
      "Train Epoch:4 [23040/60000 (38%)]\tLoss:0.005690\n",
      "Train Epoch:4 [23680/60000 (39%)]\tLoss:0.108491\n",
      "Train Epoch:4 [24320/60000 (41%)]\tLoss:0.010018\n",
      "Train Epoch:4 [24960/60000 (42%)]\tLoss:0.016571\n",
      "Train Epoch:4 [25600/60000 (43%)]\tLoss:0.028223\n",
      "Train Epoch:4 [26240/60000 (44%)]\tLoss:0.014614\n",
      "Train Epoch:4 [26880/60000 (45%)]\tLoss:0.010886\n",
      "Train Epoch:4 [27520/60000 (46%)]\tLoss:0.003959\n",
      "Train Epoch:4 [28160/60000 (47%)]\tLoss:0.079278\n",
      "Train Epoch:4 [28800/60000 (48%)]\tLoss:0.000898\n",
      "Train Epoch:4 [29440/60000 (49%)]\tLoss:0.022141\n",
      "Train Epoch:4 [30080/60000 (50%)]\tLoss:0.013625\n",
      "Train Epoch:4 [30720/60000 (51%)]\tLoss:0.061967\n",
      "Train Epoch:4 [31360/60000 (52%)]\tLoss:0.056459\n",
      "Train Epoch:4 [32000/60000 (53%)]\tLoss:0.078973\n",
      "Train Epoch:4 [32640/60000 (54%)]\tLoss:0.005224\n",
      "Train Epoch:4 [33280/60000 (55%)]\tLoss:0.000558\n",
      "Train Epoch:4 [33920/60000 (57%)]\tLoss:0.013457\n",
      "Train Epoch:4 [34560/60000 (58%)]\tLoss:0.002698\n",
      "Train Epoch:4 [35200/60000 (59%)]\tLoss:0.095655\n",
      "Train Epoch:4 [35840/60000 (60%)]\tLoss:0.041142\n",
      "Train Epoch:4 [36480/60000 (61%)]\tLoss:0.076996\n",
      "Train Epoch:4 [37120/60000 (62%)]\tLoss:0.008095\n",
      "Train Epoch:4 [37760/60000 (63%)]\tLoss:0.081091\n",
      "Train Epoch:4 [38400/60000 (64%)]\tLoss:0.051688\n",
      "Train Epoch:4 [39040/60000 (65%)]\tLoss:0.002809\n",
      "Train Epoch:4 [39680/60000 (66%)]\tLoss:0.007643\n",
      "Train Epoch:4 [40320/60000 (67%)]\tLoss:0.019246\n",
      "Train Epoch:4 [40960/60000 (68%)]\tLoss:0.086505\n",
      "Train Epoch:4 [41600/60000 (69%)]\tLoss:0.001420\n",
      "Train Epoch:4 [42240/60000 (70%)]\tLoss:0.002233\n",
      "Train Epoch:4 [42880/60000 (71%)]\tLoss:0.018819\n",
      "Train Epoch:4 [43520/60000 (72%)]\tLoss:0.024474\n",
      "Train Epoch:4 [44160/60000 (74%)]\tLoss:0.066887\n",
      "Train Epoch:4 [44800/60000 (75%)]\tLoss:0.005021\n",
      "Train Epoch:4 [45440/60000 (76%)]\tLoss:0.141754\n",
      "Train Epoch:4 [46080/60000 (77%)]\tLoss:0.051208\n",
      "Train Epoch:4 [46720/60000 (78%)]\tLoss:0.001805\n",
      "Train Epoch:4 [47360/60000 (79%)]\tLoss:0.023343\n",
      "Train Epoch:4 [48000/60000 (80%)]\tLoss:0.022859\n",
      "Train Epoch:4 [48640/60000 (81%)]\tLoss:0.003095\n",
      "Train Epoch:4 [49280/60000 (82%)]\tLoss:0.016596\n",
      "Train Epoch:4 [49920/60000 (83%)]\tLoss:0.006311\n",
      "Train Epoch:4 [50560/60000 (84%)]\tLoss:0.053822\n",
      "Train Epoch:4 [51200/60000 (85%)]\tLoss:0.016058\n",
      "Train Epoch:4 [51840/60000 (86%)]\tLoss:0.001439\n",
      "Train Epoch:4 [52480/60000 (87%)]\tLoss:0.000484\n",
      "Train Epoch:4 [53120/60000 (88%)]\tLoss:0.092816\n",
      "Train Epoch:4 [53760/60000 (90%)]\tLoss:0.002469\n",
      "Train Epoch:4 [54400/60000 (91%)]\tLoss:0.104220\n",
      "Train Epoch:4 [55040/60000 (92%)]\tLoss:0.009259\n",
      "Train Epoch:4 [55680/60000 (93%)]\tLoss:0.089553\n",
      "Train Epoch:4 [56320/60000 (94%)]\tLoss:0.007718\n",
      "Train Epoch:4 [56960/60000 (95%)]\tLoss:0.147747\n",
      "Train Epoch:4 [57600/60000 (96%)]\tLoss:0.133450\n",
      "Train Epoch:4 [58240/60000 (97%)]\tLoss:0.004895\n",
      "Train Epoch:4 [58880/60000 (98%)]\tLoss:0.128827\n",
      "Train Epoch:4 [59520/60000 (99%)]\tLoss:0.002150\n",
      "Train Epoch:5 [0/60000 (0%)]\tLoss:0.001332\n",
      "Train Epoch:5 [640/60000 (1%)]\tLoss:0.002723\n",
      "Train Epoch:5 [1280/60000 (2%)]\tLoss:0.026630\n",
      "Train Epoch:5 [1920/60000 (3%)]\tLoss:0.112846\n",
      "Train Epoch:5 [2560/60000 (4%)]\tLoss:0.004898\n",
      "Train Epoch:5 [3200/60000 (5%)]\tLoss:0.014170\n",
      "Train Epoch:5 [3840/60000 (6%)]\tLoss:0.030840\n",
      "Train Epoch:5 [4480/60000 (7%)]\tLoss:0.004083\n",
      "Train Epoch:5 [5120/60000 (9%)]\tLoss:0.000632\n",
      "Train Epoch:5 [5760/60000 (10%)]\tLoss:0.106945\n",
      "Train Epoch:5 [6400/60000 (11%)]\tLoss:0.010042\n",
      "Train Epoch:5 [7040/60000 (12%)]\tLoss:0.000938\n",
      "Train Epoch:5 [7680/60000 (13%)]\tLoss:0.024381\n",
      "Train Epoch:5 [8320/60000 (14%)]\tLoss:0.054380\n",
      "Train Epoch:5 [8960/60000 (15%)]\tLoss:0.029406\n",
      "Train Epoch:5 [9600/60000 (16%)]\tLoss:0.040188\n",
      "Train Epoch:5 [10240/60000 (17%)]\tLoss:0.005229\n",
      "Train Epoch:5 [10880/60000 (18%)]\tLoss:0.006207\n",
      "Train Epoch:5 [11520/60000 (19%)]\tLoss:0.000745\n",
      "Train Epoch:5 [12160/60000 (20%)]\tLoss:0.037868\n",
      "Train Epoch:5 [12800/60000 (21%)]\tLoss:0.110130\n",
      "Train Epoch:5 [13440/60000 (22%)]\tLoss:0.012335\n",
      "Train Epoch:5 [14080/60000 (23%)]\tLoss:0.011735\n",
      "Train Epoch:5 [14720/60000 (25%)]\tLoss:0.068329\n",
      "Train Epoch:5 [15360/60000 (26%)]\tLoss:0.002727\n",
      "Train Epoch:5 [16000/60000 (27%)]\tLoss:0.071820\n",
      "Train Epoch:5 [16640/60000 (28%)]\tLoss:0.007949\n",
      "Train Epoch:5 [17280/60000 (29%)]\tLoss:0.033049\n",
      "Train Epoch:5 [17920/60000 (30%)]\tLoss:0.173875\n",
      "Train Epoch:5 [18560/60000 (31%)]\tLoss:0.019321\n",
      "Train Epoch:5 [19200/60000 (32%)]\tLoss:0.050320\n",
      "Train Epoch:5 [19840/60000 (33%)]\tLoss:0.023193\n",
      "Train Epoch:5 [20480/60000 (34%)]\tLoss:0.001031\n",
      "Train Epoch:5 [21120/60000 (35%)]\tLoss:0.001009\n",
      "Train Epoch:5 [21760/60000 (36%)]\tLoss:0.012020\n",
      "Train Epoch:5 [22400/60000 (37%)]\tLoss:0.003615\n",
      "Train Epoch:5 [23040/60000 (38%)]\tLoss:0.036221\n",
      "Train Epoch:5 [23680/60000 (39%)]\tLoss:0.000270\n",
      "Train Epoch:5 [24320/60000 (41%)]\tLoss:0.003903\n",
      "Train Epoch:5 [24960/60000 (42%)]\tLoss:0.051474\n",
      "Train Epoch:5 [25600/60000 (43%)]\tLoss:0.002435\n",
      "Train Epoch:5 [26240/60000 (44%)]\tLoss:0.009791\n",
      "Train Epoch:5 [26880/60000 (45%)]\tLoss:0.006419\n",
      "Train Epoch:5 [27520/60000 (46%)]\tLoss:0.017517\n",
      "Train Epoch:5 [28160/60000 (47%)]\tLoss:0.108978\n",
      "Train Epoch:5 [28800/60000 (48%)]\tLoss:0.003606\n",
      "Train Epoch:5 [29440/60000 (49%)]\tLoss:0.003531\n",
      "Train Epoch:5 [30080/60000 (50%)]\tLoss:0.025305\n",
      "Train Epoch:5 [30720/60000 (51%)]\tLoss:0.018505\n",
      "Train Epoch:5 [31360/60000 (52%)]\tLoss:0.003170\n",
      "Train Epoch:5 [32000/60000 (53%)]\tLoss:0.001660\n",
      "Train Epoch:5 [32640/60000 (54%)]\tLoss:0.004645\n",
      "Train Epoch:5 [33280/60000 (55%)]\tLoss:0.021298\n",
      "Train Epoch:5 [33920/60000 (57%)]\tLoss:0.003463\n",
      "Train Epoch:5 [34560/60000 (58%)]\tLoss:0.015886\n",
      "Train Epoch:5 [35200/60000 (59%)]\tLoss:0.017613\n",
      "Train Epoch:5 [35840/60000 (60%)]\tLoss:0.040200\n",
      "Train Epoch:5 [36480/60000 (61%)]\tLoss:0.013821\n",
      "Train Epoch:5 [37120/60000 (62%)]\tLoss:0.009077\n",
      "Train Epoch:5 [37760/60000 (63%)]\tLoss:0.032401\n",
      "Train Epoch:5 [38400/60000 (64%)]\tLoss:0.072090\n",
      "Train Epoch:5 [39040/60000 (65%)]\tLoss:0.017634\n",
      "Train Epoch:5 [39680/60000 (66%)]\tLoss:0.146146\n",
      "Train Epoch:5 [40320/60000 (67%)]\tLoss:0.094114\n",
      "Train Epoch:5 [40960/60000 (68%)]\tLoss:0.021465\n",
      "Train Epoch:5 [41600/60000 (69%)]\tLoss:0.117382\n",
      "Train Epoch:5 [42240/60000 (70%)]\tLoss:0.000619\n",
      "Train Epoch:5 [42880/60000 (71%)]\tLoss:0.013225\n",
      "Train Epoch:5 [43520/60000 (72%)]\tLoss:0.042470\n",
      "Train Epoch:5 [44160/60000 (74%)]\tLoss:0.092977\n",
      "Train Epoch:5 [44800/60000 (75%)]\tLoss:0.001795\n",
      "Train Epoch:5 [45440/60000 (76%)]\tLoss:0.005134\n",
      "Train Epoch:5 [46080/60000 (77%)]\tLoss:0.006900\n",
      "Train Epoch:5 [46720/60000 (78%)]\tLoss:0.001801\n",
      "Train Epoch:5 [47360/60000 (79%)]\tLoss:0.000935\n",
      "Train Epoch:5 [48000/60000 (80%)]\tLoss:0.000585\n",
      "Train Epoch:5 [48640/60000 (81%)]\tLoss:0.003799\n",
      "Train Epoch:5 [49280/60000 (82%)]\tLoss:0.007413\n",
      "Train Epoch:5 [49920/60000 (83%)]\tLoss:0.011667\n",
      "Train Epoch:5 [50560/60000 (84%)]\tLoss:0.002263\n",
      "Train Epoch:5 [51200/60000 (85%)]\tLoss:0.033848\n",
      "Train Epoch:5 [51840/60000 (86%)]\tLoss:0.058822\n",
      "Train Epoch:5 [52480/60000 (87%)]\tLoss:0.088933\n",
      "Train Epoch:5 [53120/60000 (88%)]\tLoss:0.093559\n",
      "Train Epoch:5 [53760/60000 (90%)]\tLoss:0.028579\n",
      "Train Epoch:5 [54400/60000 (91%)]\tLoss:0.002026\n",
      "Train Epoch:5 [55040/60000 (92%)]\tLoss:0.000838\n",
      "Train Epoch:5 [55680/60000 (93%)]\tLoss:0.026379\n",
      "Train Epoch:5 [56320/60000 (94%)]\tLoss:0.001269\n",
      "Train Epoch:5 [56960/60000 (95%)]\tLoss:0.000635\n",
      "Train Epoch:5 [57600/60000 (96%)]\tLoss:0.002782\n",
      "Train Epoch:5 [58240/60000 (97%)]\tLoss:0.007763\n",
      "Train Epoch:5 [58880/60000 (98%)]\tLoss:0.091451\n",
      "Train Epoch:5 [59520/60000 (99%)]\tLoss:0.008515\n",
      "Train Epoch:6 [0/60000 (0%)]\tLoss:0.032566\n",
      "Train Epoch:6 [640/60000 (1%)]\tLoss:0.120779\n",
      "Train Epoch:6 [1280/60000 (2%)]\tLoss:0.016940\n",
      "Train Epoch:6 [1920/60000 (3%)]\tLoss:0.031869\n",
      "Train Epoch:6 [2560/60000 (4%)]\tLoss:0.033725\n",
      "Train Epoch:6 [3200/60000 (5%)]\tLoss:0.000773\n",
      "Train Epoch:6 [3840/60000 (6%)]\tLoss:0.000966\n",
      "Train Epoch:6 [4480/60000 (7%)]\tLoss:0.054735\n",
      "Train Epoch:6 [5120/60000 (9%)]\tLoss:0.002747\n",
      "Train Epoch:6 [5760/60000 (10%)]\tLoss:0.018954\n",
      "Train Epoch:6 [6400/60000 (11%)]\tLoss:0.003588\n",
      "Train Epoch:6 [7040/60000 (12%)]\tLoss:0.015526\n",
      "Train Epoch:6 [7680/60000 (13%)]\tLoss:0.013024\n",
      "Train Epoch:6 [8320/60000 (14%)]\tLoss:0.011768\n",
      "Train Epoch:6 [8960/60000 (15%)]\tLoss:0.000980\n",
      "Train Epoch:6 [9600/60000 (16%)]\tLoss:0.007156\n",
      "Train Epoch:6 [10240/60000 (17%)]\tLoss:0.004209\n",
      "Train Epoch:6 [10880/60000 (18%)]\tLoss:0.093290\n",
      "Train Epoch:6 [11520/60000 (19%)]\tLoss:0.017633\n",
      "Train Epoch:6 [12160/60000 (20%)]\tLoss:0.005778\n",
      "Train Epoch:6 [12800/60000 (21%)]\tLoss:0.086092\n",
      "Train Epoch:6 [13440/60000 (22%)]\tLoss:0.130145\n",
      "Train Epoch:6 [14080/60000 (23%)]\tLoss:0.093127\n",
      "Train Epoch:6 [14720/60000 (25%)]\tLoss:0.017181\n",
      "Train Epoch:6 [15360/60000 (26%)]\tLoss:0.003409\n",
      "Train Epoch:6 [16000/60000 (27%)]\tLoss:0.002347\n",
      "Train Epoch:6 [16640/60000 (28%)]\tLoss:0.002697\n",
      "Train Epoch:6 [17280/60000 (29%)]\tLoss:0.122106\n",
      "Train Epoch:6 [17920/60000 (30%)]\tLoss:0.035068\n",
      "Train Epoch:6 [18560/60000 (31%)]\tLoss:0.077548\n",
      "Train Epoch:6 [19200/60000 (32%)]\tLoss:0.035110\n",
      "Train Epoch:6 [19840/60000 (33%)]\tLoss:0.019479\n",
      "Train Epoch:6 [20480/60000 (34%)]\tLoss:0.013680\n",
      "Train Epoch:6 [21120/60000 (35%)]\tLoss:0.003758\n",
      "Train Epoch:6 [21760/60000 (36%)]\tLoss:0.002236\n",
      "Train Epoch:6 [22400/60000 (37%)]\tLoss:0.001001\n",
      "Train Epoch:6 [23040/60000 (38%)]\tLoss:0.042696\n",
      "Train Epoch:6 [23680/60000 (39%)]\tLoss:0.000119\n",
      "Train Epoch:6 [24320/60000 (41%)]\tLoss:0.017117\n",
      "Train Epoch:6 [24960/60000 (42%)]\tLoss:0.026289\n",
      "Train Epoch:6 [25600/60000 (43%)]\tLoss:0.092275\n",
      "Train Epoch:6 [26240/60000 (44%)]\tLoss:0.018846\n",
      "Train Epoch:6 [26880/60000 (45%)]\tLoss:0.002788\n",
      "Train Epoch:6 [27520/60000 (46%)]\tLoss:0.003884\n",
      "Train Epoch:6 [28160/60000 (47%)]\tLoss:0.006572\n",
      "Train Epoch:6 [28800/60000 (48%)]\tLoss:0.024835\n",
      "Train Epoch:6 [29440/60000 (49%)]\tLoss:0.037898\n",
      "Train Epoch:6 [30080/60000 (50%)]\tLoss:0.006564\n",
      "Train Epoch:6 [30720/60000 (51%)]\tLoss:0.012440\n",
      "Train Epoch:6 [31360/60000 (52%)]\tLoss:0.021478\n",
      "Train Epoch:6 [32000/60000 (53%)]\tLoss:0.001395\n",
      "Train Epoch:6 [32640/60000 (54%)]\tLoss:0.005293\n",
      "Train Epoch:6 [33280/60000 (55%)]\tLoss:0.009636\n",
      "Train Epoch:6 [33920/60000 (57%)]\tLoss:0.050688\n",
      "Train Epoch:6 [34560/60000 (58%)]\tLoss:0.067496\n",
      "Train Epoch:6 [35200/60000 (59%)]\tLoss:0.004936\n",
      "Train Epoch:6 [35840/60000 (60%)]\tLoss:0.055161\n",
      "Train Epoch:6 [36480/60000 (61%)]\tLoss:0.030784\n",
      "Train Epoch:6 [37120/60000 (62%)]\tLoss:0.038523\n",
      "Train Epoch:6 [37760/60000 (63%)]\tLoss:0.012435\n",
      "Train Epoch:6 [38400/60000 (64%)]\tLoss:0.005973\n",
      "Train Epoch:6 [39040/60000 (65%)]\tLoss:0.024185\n",
      "Train Epoch:6 [39680/60000 (66%)]\tLoss:0.054995\n",
      "Train Epoch:6 [40320/60000 (67%)]\tLoss:0.007401\n",
      "Train Epoch:6 [40960/60000 (68%)]\tLoss:0.005769\n",
      "Train Epoch:6 [41600/60000 (69%)]\tLoss:0.001331\n",
      "Train Epoch:6 [42240/60000 (70%)]\tLoss:0.001707\n",
      "Train Epoch:6 [42880/60000 (71%)]\tLoss:0.001680\n",
      "Train Epoch:6 [43520/60000 (72%)]\tLoss:0.001887\n",
      "Train Epoch:6 [44160/60000 (74%)]\tLoss:0.001337\n",
      "Train Epoch:6 [44800/60000 (75%)]\tLoss:0.000197\n",
      "Train Epoch:6 [45440/60000 (76%)]\tLoss:0.011075\n",
      "Train Epoch:6 [46080/60000 (77%)]\tLoss:0.012041\n",
      "Train Epoch:6 [46720/60000 (78%)]\tLoss:0.002451\n",
      "Train Epoch:6 [47360/60000 (79%)]\tLoss:0.012459\n",
      "Train Epoch:6 [48000/60000 (80%)]\tLoss:0.001057\n",
      "Train Epoch:6 [48640/60000 (81%)]\tLoss:0.031113\n",
      "Train Epoch:6 [49280/60000 (82%)]\tLoss:0.013149\n",
      "Train Epoch:6 [49920/60000 (83%)]\tLoss:0.021572\n",
      "Train Epoch:6 [50560/60000 (84%)]\tLoss:0.011308\n",
      "Train Epoch:6 [51200/60000 (85%)]\tLoss:0.160733\n",
      "Train Epoch:6 [51840/60000 (86%)]\tLoss:0.005750\n",
      "Train Epoch:6 [52480/60000 (87%)]\tLoss:0.091390\n",
      "Train Epoch:6 [53120/60000 (88%)]\tLoss:0.029440\n",
      "Train Epoch:6 [53760/60000 (90%)]\tLoss:0.000720\n",
      "Train Epoch:6 [54400/60000 (91%)]\tLoss:0.027558\n",
      "Train Epoch:6 [55040/60000 (92%)]\tLoss:0.098507\n",
      "Train Epoch:6 [55680/60000 (93%)]\tLoss:0.004660\n",
      "Train Epoch:6 [56320/60000 (94%)]\tLoss:0.043716\n",
      "Train Epoch:6 [56960/60000 (95%)]\tLoss:0.000232\n",
      "Train Epoch:6 [57600/60000 (96%)]\tLoss:0.005019\n",
      "Train Epoch:6 [58240/60000 (97%)]\tLoss:0.112435\n",
      "Train Epoch:6 [58880/60000 (98%)]\tLoss:0.005658\n",
      "Train Epoch:6 [59520/60000 (99%)]\tLoss:0.011227\n",
      "Train Epoch:7 [0/60000 (0%)]\tLoss:0.049554\n",
      "Train Epoch:7 [640/60000 (1%)]\tLoss:0.001797\n",
      "Train Epoch:7 [1280/60000 (2%)]\tLoss:0.017184\n",
      "Train Epoch:7 [1920/60000 (3%)]\tLoss:0.009209\n",
      "Train Epoch:7 [2560/60000 (4%)]\tLoss:0.002309\n",
      "Train Epoch:7 [3200/60000 (5%)]\tLoss:0.002287\n",
      "Train Epoch:7 [3840/60000 (6%)]\tLoss:0.062638\n",
      "Train Epoch:7 [4480/60000 (7%)]\tLoss:0.008486\n",
      "Train Epoch:7 [5120/60000 (9%)]\tLoss:0.004320\n",
      "Train Epoch:7 [5760/60000 (10%)]\tLoss:0.025008\n",
      "Train Epoch:7 [6400/60000 (11%)]\tLoss:0.001815\n",
      "Train Epoch:7 [7040/60000 (12%)]\tLoss:0.000569\n",
      "Train Epoch:7 [7680/60000 (13%)]\tLoss:0.000409\n",
      "Train Epoch:7 [8320/60000 (14%)]\tLoss:0.001718\n",
      "Train Epoch:7 [8960/60000 (15%)]\tLoss:0.000892\n",
      "Train Epoch:7 [9600/60000 (16%)]\tLoss:0.006902\n",
      "Train Epoch:7 [10240/60000 (17%)]\tLoss:0.057142\n",
      "Train Epoch:7 [10880/60000 (18%)]\tLoss:0.011546\n",
      "Train Epoch:7 [11520/60000 (19%)]\tLoss:0.000563\n",
      "Train Epoch:7 [12160/60000 (20%)]\tLoss:0.076110\n",
      "Train Epoch:7 [12800/60000 (21%)]\tLoss:0.004211\n",
      "Train Epoch:7 [13440/60000 (22%)]\tLoss:0.000497\n",
      "Train Epoch:7 [14080/60000 (23%)]\tLoss:0.075060\n",
      "Train Epoch:7 [14720/60000 (25%)]\tLoss:0.000269\n",
      "Train Epoch:7 [15360/60000 (26%)]\tLoss:0.023480\n",
      "Train Epoch:7 [16000/60000 (27%)]\tLoss:0.005909\n",
      "Train Epoch:7 [16640/60000 (28%)]\tLoss:0.038967\n",
      "Train Epoch:7 [17280/60000 (29%)]\tLoss:0.000964\n",
      "Train Epoch:7 [17920/60000 (30%)]\tLoss:0.056412\n",
      "Train Epoch:7 [18560/60000 (31%)]\tLoss:0.000372\n",
      "Train Epoch:7 [19200/60000 (32%)]\tLoss:0.103318\n",
      "Train Epoch:7 [19840/60000 (33%)]\tLoss:0.044841\n",
      "Train Epoch:7 [20480/60000 (34%)]\tLoss:0.016337\n",
      "Train Epoch:7 [21120/60000 (35%)]\tLoss:0.005312\n",
      "Train Epoch:7 [21760/60000 (36%)]\tLoss:0.018224\n",
      "Train Epoch:7 [22400/60000 (37%)]\tLoss:0.002814\n",
      "Train Epoch:7 [23040/60000 (38%)]\tLoss:0.001609\n",
      "Train Epoch:7 [23680/60000 (39%)]\tLoss:0.073127\n",
      "Train Epoch:7 [24320/60000 (41%)]\tLoss:0.007473\n",
      "Train Epoch:7 [24960/60000 (42%)]\tLoss:0.003519\n",
      "Train Epoch:7 [25600/60000 (43%)]\tLoss:0.103501\n",
      "Train Epoch:7 [26240/60000 (44%)]\tLoss:0.005172\n",
      "Train Epoch:7 [26880/60000 (45%)]\tLoss:0.129265\n",
      "Train Epoch:7 [27520/60000 (46%)]\tLoss:0.122128\n",
      "Train Epoch:7 [28160/60000 (47%)]\tLoss:0.003100\n",
      "Train Epoch:7 [28800/60000 (48%)]\tLoss:0.027483\n",
      "Train Epoch:7 [29440/60000 (49%)]\tLoss:0.005526\n",
      "Train Epoch:7 [30080/60000 (50%)]\tLoss:0.007451\n",
      "Train Epoch:7 [30720/60000 (51%)]\tLoss:0.036509\n",
      "Train Epoch:7 [31360/60000 (52%)]\tLoss:0.001009\n",
      "Train Epoch:7 [32000/60000 (53%)]\tLoss:0.000479\n",
      "Train Epoch:7 [32640/60000 (54%)]\tLoss:0.022738\n",
      "Train Epoch:7 [33280/60000 (55%)]\tLoss:0.052121\n",
      "Train Epoch:7 [33920/60000 (57%)]\tLoss:0.004917\n",
      "Train Epoch:7 [34560/60000 (58%)]\tLoss:0.001855\n",
      "Train Epoch:7 [35200/60000 (59%)]\tLoss:0.004809\n",
      "Train Epoch:7 [35840/60000 (60%)]\tLoss:0.017144\n",
      "Train Epoch:7 [36480/60000 (61%)]\tLoss:0.027598\n",
      "Train Epoch:7 [37120/60000 (62%)]\tLoss:0.110946\n",
      "Train Epoch:7 [37760/60000 (63%)]\tLoss:0.019656\n",
      "Train Epoch:7 [38400/60000 (64%)]\tLoss:0.016325\n",
      "Train Epoch:7 [39040/60000 (65%)]\tLoss:0.000649\n",
      "Train Epoch:7 [39680/60000 (66%)]\tLoss:0.016729\n",
      "Train Epoch:7 [40320/60000 (67%)]\tLoss:0.000927\n",
      "Train Epoch:7 [40960/60000 (68%)]\tLoss:0.000868\n",
      "Train Epoch:7 [41600/60000 (69%)]\tLoss:0.000220\n",
      "Train Epoch:7 [42240/60000 (70%)]\tLoss:0.008090\n",
      "Train Epoch:7 [42880/60000 (71%)]\tLoss:0.012814\n",
      "Train Epoch:7 [43520/60000 (72%)]\tLoss:0.058468\n",
      "Train Epoch:7 [44160/60000 (74%)]\tLoss:0.021901\n",
      "Train Epoch:7 [44800/60000 (75%)]\tLoss:0.082349\n",
      "Train Epoch:7 [45440/60000 (76%)]\tLoss:0.051289\n",
      "Train Epoch:7 [46080/60000 (77%)]\tLoss:0.007144\n",
      "Train Epoch:7 [46720/60000 (78%)]\tLoss:0.028923\n",
      "Train Epoch:7 [47360/60000 (79%)]\tLoss:0.003436\n",
      "Train Epoch:7 [48000/60000 (80%)]\tLoss:0.076928\n",
      "Train Epoch:7 [48640/60000 (81%)]\tLoss:0.021687\n",
      "Train Epoch:7 [49280/60000 (82%)]\tLoss:0.000396\n",
      "Train Epoch:7 [49920/60000 (83%)]\tLoss:0.117634\n",
      "Train Epoch:7 [50560/60000 (84%)]\tLoss:0.008920\n",
      "Train Epoch:7 [51200/60000 (85%)]\tLoss:0.000171\n",
      "Train Epoch:7 [51840/60000 (86%)]\tLoss:0.002580\n",
      "Train Epoch:7 [52480/60000 (87%)]\tLoss:0.024923\n",
      "Train Epoch:7 [53120/60000 (88%)]\tLoss:0.003814\n",
      "Train Epoch:7 [53760/60000 (90%)]\tLoss:0.001146\n",
      "Train Epoch:7 [54400/60000 (91%)]\tLoss:0.000182\n",
      "Train Epoch:7 [55040/60000 (92%)]\tLoss:0.007201\n",
      "Train Epoch:7 [55680/60000 (93%)]\tLoss:0.004529\n",
      "Train Epoch:7 [56320/60000 (94%)]\tLoss:0.046299\n",
      "Train Epoch:7 [56960/60000 (95%)]\tLoss:0.035940\n",
      "Train Epoch:7 [57600/60000 (96%)]\tLoss:0.013513\n",
      "Train Epoch:7 [58240/60000 (97%)]\tLoss:0.052991\n",
      "Train Epoch:7 [58880/60000 (98%)]\tLoss:0.000473\n",
      "Train Epoch:7 [59520/60000 (99%)]\tLoss:0.003660\n",
      "Train Epoch:8 [0/60000 (0%)]\tLoss:0.227411\n",
      "Train Epoch:8 [640/60000 (1%)]\tLoss:0.112691\n",
      "Train Epoch:8 [1280/60000 (2%)]\tLoss:0.036993\n",
      "Train Epoch:8 [1920/60000 (3%)]\tLoss:0.021449\n",
      "Train Epoch:8 [2560/60000 (4%)]\tLoss:0.013029\n",
      "Train Epoch:8 [3200/60000 (5%)]\tLoss:0.001069\n",
      "Train Epoch:8 [3840/60000 (6%)]\tLoss:0.042029\n",
      "Train Epoch:8 [4480/60000 (7%)]\tLoss:0.026348\n",
      "Train Epoch:8 [5120/60000 (9%)]\tLoss:0.004959\n",
      "Train Epoch:8 [5760/60000 (10%)]\tLoss:0.007456\n",
      "Train Epoch:8 [6400/60000 (11%)]\tLoss:0.001702\n",
      "Train Epoch:8 [7040/60000 (12%)]\tLoss:0.002486\n",
      "Train Epoch:8 [7680/60000 (13%)]\tLoss:0.002721\n",
      "Train Epoch:8 [8320/60000 (14%)]\tLoss:0.056877\n",
      "Train Epoch:8 [8960/60000 (15%)]\tLoss:0.206482\n",
      "Train Epoch:8 [9600/60000 (16%)]\tLoss:0.019118\n",
      "Train Epoch:8 [10240/60000 (17%)]\tLoss:0.000226\n",
      "Train Epoch:8 [10880/60000 (18%)]\tLoss:0.154696\n",
      "Train Epoch:8 [11520/60000 (19%)]\tLoss:0.046322\n",
      "Train Epoch:8 [12160/60000 (20%)]\tLoss:0.014752\n",
      "Train Epoch:8 [12800/60000 (21%)]\tLoss:0.002964\n",
      "Train Epoch:8 [13440/60000 (22%)]\tLoss:0.011942\n",
      "Train Epoch:8 [14080/60000 (23%)]\tLoss:0.008795\n",
      "Train Epoch:8 [14720/60000 (25%)]\tLoss:0.074559\n",
      "Train Epoch:8 [15360/60000 (26%)]\tLoss:0.004647\n",
      "Train Epoch:8 [16000/60000 (27%)]\tLoss:0.088180\n",
      "Train Epoch:8 [16640/60000 (28%)]\tLoss:0.000383\n",
      "Train Epoch:8 [17280/60000 (29%)]\tLoss:0.011427\n",
      "Train Epoch:8 [17920/60000 (30%)]\tLoss:0.020628\n",
      "Train Epoch:8 [18560/60000 (31%)]\tLoss:0.006498\n",
      "Train Epoch:8 [19200/60000 (32%)]\tLoss:0.002780\n",
      "Train Epoch:8 [19840/60000 (33%)]\tLoss:0.016855\n",
      "Train Epoch:8 [20480/60000 (34%)]\tLoss:0.011506\n",
      "Train Epoch:8 [21120/60000 (35%)]\tLoss:0.020282\n",
      "Train Epoch:8 [21760/60000 (36%)]\tLoss:0.052116\n",
      "Train Epoch:8 [22400/60000 (37%)]\tLoss:0.035231\n",
      "Train Epoch:8 [23040/60000 (38%)]\tLoss:0.000346\n",
      "Train Epoch:8 [23680/60000 (39%)]\tLoss:0.067750\n",
      "Train Epoch:8 [24320/60000 (41%)]\tLoss:0.091793\n",
      "Train Epoch:8 [24960/60000 (42%)]\tLoss:0.009170\n",
      "Train Epoch:8 [25600/60000 (43%)]\tLoss:0.000667\n",
      "Train Epoch:8 [26240/60000 (44%)]\tLoss:0.004375\n",
      "Train Epoch:8 [26880/60000 (45%)]\tLoss:0.008515\n",
      "Train Epoch:8 [27520/60000 (46%)]\tLoss:0.014334\n",
      "Train Epoch:8 [28160/60000 (47%)]\tLoss:0.000359\n",
      "Train Epoch:8 [28800/60000 (48%)]\tLoss:0.007215\n",
      "Train Epoch:8 [29440/60000 (49%)]\tLoss:0.000062\n",
      "Train Epoch:8 [30080/60000 (50%)]\tLoss:0.045454\n",
      "Train Epoch:8 [30720/60000 (51%)]\tLoss:0.000654\n",
      "Train Epoch:8 [31360/60000 (52%)]\tLoss:0.002915\n",
      "Train Epoch:8 [32000/60000 (53%)]\tLoss:0.003389\n",
      "Train Epoch:8 [32640/60000 (54%)]\tLoss:0.022651\n",
      "Train Epoch:8 [33280/60000 (55%)]\tLoss:0.013619\n",
      "Train Epoch:8 [33920/60000 (57%)]\tLoss:0.048875\n",
      "Train Epoch:8 [34560/60000 (58%)]\tLoss:0.137429\n",
      "Train Epoch:8 [35200/60000 (59%)]\tLoss:0.002335\n",
      "Train Epoch:8 [35840/60000 (60%)]\tLoss:0.001061\n",
      "Train Epoch:8 [36480/60000 (61%)]\tLoss:0.004220\n",
      "Train Epoch:8 [37120/60000 (62%)]\tLoss:0.005637\n",
      "Train Epoch:8 [37760/60000 (63%)]\tLoss:0.002505\n",
      "Train Epoch:8 [38400/60000 (64%)]\tLoss:0.003632\n",
      "Train Epoch:8 [39040/60000 (65%)]\tLoss:0.003965\n",
      "Train Epoch:8 [39680/60000 (66%)]\tLoss:0.028237\n",
      "Train Epoch:8 [40320/60000 (67%)]\tLoss:0.214819\n",
      "Train Epoch:8 [40960/60000 (68%)]\tLoss:0.007422\n",
      "Train Epoch:8 [41600/60000 (69%)]\tLoss:0.000459\n",
      "Train Epoch:8 [42240/60000 (70%)]\tLoss:0.012308\n",
      "Train Epoch:8 [42880/60000 (71%)]\tLoss:0.017769\n",
      "Train Epoch:8 [43520/60000 (72%)]\tLoss:0.000991\n",
      "Train Epoch:8 [44160/60000 (74%)]\tLoss:0.000478\n",
      "Train Epoch:8 [44800/60000 (75%)]\tLoss:0.021478\n",
      "Train Epoch:8 [45440/60000 (76%)]\tLoss:0.027856\n",
      "Train Epoch:8 [46080/60000 (77%)]\tLoss:0.001589\n",
      "Train Epoch:8 [46720/60000 (78%)]\tLoss:0.003121\n",
      "Train Epoch:8 [47360/60000 (79%)]\tLoss:0.000700\n",
      "Train Epoch:8 [48000/60000 (80%)]\tLoss:0.008516\n",
      "Train Epoch:8 [48640/60000 (81%)]\tLoss:0.039184\n",
      "Train Epoch:8 [49280/60000 (82%)]\tLoss:0.021132\n",
      "Train Epoch:8 [49920/60000 (83%)]\tLoss:0.001685\n",
      "Train Epoch:8 [50560/60000 (84%)]\tLoss:0.119858\n",
      "Train Epoch:8 [51200/60000 (85%)]\tLoss:0.015894\n",
      "Train Epoch:8 [51840/60000 (86%)]\tLoss:0.103869\n",
      "Train Epoch:8 [52480/60000 (87%)]\tLoss:0.001614\n",
      "Train Epoch:8 [53120/60000 (88%)]\tLoss:0.066517\n",
      "Train Epoch:8 [53760/60000 (90%)]\tLoss:0.032445\n",
      "Train Epoch:8 [54400/60000 (91%)]\tLoss:0.035795\n",
      "Train Epoch:8 [55040/60000 (92%)]\tLoss:0.006468\n",
      "Train Epoch:8 [55680/60000 (93%)]\tLoss:0.004688\n",
      "Train Epoch:8 [56320/60000 (94%)]\tLoss:0.001251\n",
      "Train Epoch:8 [56960/60000 (95%)]\tLoss:0.002014\n",
      "Train Epoch:8 [57600/60000 (96%)]\tLoss:0.023382\n",
      "Train Epoch:8 [58240/60000 (97%)]\tLoss:0.026533\n",
      "Train Epoch:8 [58880/60000 (98%)]\tLoss:0.000328\n",
      "Train Epoch:8 [59520/60000 (99%)]\tLoss:0.003666\n",
      "Train Epoch:9 [0/60000 (0%)]\tLoss:0.005765\n",
      "Train Epoch:9 [640/60000 (1%)]\tLoss:0.069794\n",
      "Train Epoch:9 [1280/60000 (2%)]\tLoss:0.016976\n",
      "Train Epoch:9 [1920/60000 (3%)]\tLoss:0.007052\n",
      "Train Epoch:9 [2560/60000 (4%)]\tLoss:0.000299\n",
      "Train Epoch:9 [3200/60000 (5%)]\tLoss:0.001402\n",
      "Train Epoch:9 [3840/60000 (6%)]\tLoss:0.000778\n",
      "Train Epoch:9 [4480/60000 (7%)]\tLoss:0.044702\n",
      "Train Epoch:9 [5120/60000 (9%)]\tLoss:0.033338\n",
      "Train Epoch:9 [5760/60000 (10%)]\tLoss:0.000469\n",
      "Train Epoch:9 [6400/60000 (11%)]\tLoss:0.001807\n",
      "Train Epoch:9 [7040/60000 (12%)]\tLoss:0.000701\n",
      "Train Epoch:9 [7680/60000 (13%)]\tLoss:0.001631\n",
      "Train Epoch:9 [8320/60000 (14%)]\tLoss:0.001842\n",
      "Train Epoch:9 [8960/60000 (15%)]\tLoss:0.000233\n",
      "Train Epoch:9 [9600/60000 (16%)]\tLoss:0.094635\n",
      "Train Epoch:9 [10240/60000 (17%)]\tLoss:0.038218\n",
      "Train Epoch:9 [10880/60000 (18%)]\tLoss:0.026300\n",
      "Train Epoch:9 [11520/60000 (19%)]\tLoss:0.000234\n",
      "Train Epoch:9 [12160/60000 (20%)]\tLoss:0.103012\n",
      "Train Epoch:9 [12800/60000 (21%)]\tLoss:0.030481\n",
      "Train Epoch:9 [13440/60000 (22%)]\tLoss:0.011032\n",
      "Train Epoch:9 [14080/60000 (23%)]\tLoss:0.002528\n",
      "Train Epoch:9 [14720/60000 (25%)]\tLoss:0.007921\n",
      "Train Epoch:9 [15360/60000 (26%)]\tLoss:0.085486\n",
      "Train Epoch:9 [16000/60000 (27%)]\tLoss:0.008958\n",
      "Train Epoch:9 [16640/60000 (28%)]\tLoss:0.016078\n",
      "Train Epoch:9 [17280/60000 (29%)]\tLoss:0.001956\n",
      "Train Epoch:9 [17920/60000 (30%)]\tLoss:0.005395\n",
      "Train Epoch:9 [18560/60000 (31%)]\tLoss:0.003514\n",
      "Train Epoch:9 [19200/60000 (32%)]\tLoss:0.001167\n",
      "Train Epoch:9 [19840/60000 (33%)]\tLoss:0.044048\n",
      "Train Epoch:9 [20480/60000 (34%)]\tLoss:0.029297\n",
      "Train Epoch:9 [21120/60000 (35%)]\tLoss:0.002951\n",
      "Train Epoch:9 [21760/60000 (36%)]\tLoss:0.001914\n",
      "Train Epoch:9 [22400/60000 (37%)]\tLoss:0.034376\n",
      "Train Epoch:9 [23040/60000 (38%)]\tLoss:0.039885\n",
      "Train Epoch:9 [23680/60000 (39%)]\tLoss:0.006355\n",
      "Train Epoch:9 [24320/60000 (41%)]\tLoss:0.003144\n",
      "Train Epoch:9 [24960/60000 (42%)]\tLoss:0.046357\n",
      "Train Epoch:9 [25600/60000 (43%)]\tLoss:0.032602\n",
      "Train Epoch:9 [26240/60000 (44%)]\tLoss:0.068641\n",
      "Train Epoch:9 [26880/60000 (45%)]\tLoss:0.112499\n",
      "Train Epoch:9 [27520/60000 (46%)]\tLoss:0.008449\n",
      "Train Epoch:9 [28160/60000 (47%)]\tLoss:0.014650\n",
      "Train Epoch:9 [28800/60000 (48%)]\tLoss:0.000493\n",
      "Train Epoch:9 [29440/60000 (49%)]\tLoss:0.000182\n",
      "Train Epoch:9 [30080/60000 (50%)]\tLoss:0.003728\n",
      "Train Epoch:9 [30720/60000 (51%)]\tLoss:0.000896\n",
      "Train Epoch:9 [31360/60000 (52%)]\tLoss:0.006812\n",
      "Train Epoch:9 [32000/60000 (53%)]\tLoss:0.041634\n",
      "Train Epoch:9 [32640/60000 (54%)]\tLoss:0.073375\n",
      "Train Epoch:9 [33280/60000 (55%)]\tLoss:0.005079\n",
      "Train Epoch:9 [33920/60000 (57%)]\tLoss:0.032594\n",
      "Train Epoch:9 [34560/60000 (58%)]\tLoss:0.008449\n",
      "Train Epoch:9 [35200/60000 (59%)]\tLoss:0.004497\n",
      "Train Epoch:9 [35840/60000 (60%)]\tLoss:0.003503\n",
      "Train Epoch:9 [36480/60000 (61%)]\tLoss:0.007895\n",
      "Train Epoch:9 [37120/60000 (62%)]\tLoss:0.020377\n",
      "Train Epoch:9 [37760/60000 (63%)]\tLoss:0.003495\n",
      "Train Epoch:9 [38400/60000 (64%)]\tLoss:0.009066\n",
      "Train Epoch:9 [39040/60000 (65%)]\tLoss:0.011644\n",
      "Train Epoch:9 [39680/60000 (66%)]\tLoss:0.000926\n",
      "Train Epoch:9 [40320/60000 (67%)]\tLoss:0.000963\n",
      "Train Epoch:9 [40960/60000 (68%)]\tLoss:0.023511\n",
      "Train Epoch:9 [41600/60000 (69%)]\tLoss:0.000849\n",
      "Train Epoch:9 [42240/60000 (70%)]\tLoss:0.112628\n",
      "Train Epoch:9 [42880/60000 (71%)]\tLoss:0.006823\n",
      "Train Epoch:9 [43520/60000 (72%)]\tLoss:0.044887\n",
      "Train Epoch:9 [44160/60000 (74%)]\tLoss:0.022911\n",
      "Train Epoch:9 [44800/60000 (75%)]\tLoss:0.001235\n",
      "Train Epoch:9 [45440/60000 (76%)]\tLoss:0.021434\n",
      "Train Epoch:9 [46080/60000 (77%)]\tLoss:0.001861\n",
      "Train Epoch:9 [46720/60000 (78%)]\tLoss:0.000669\n",
      "Train Epoch:9 [47360/60000 (79%)]\tLoss:0.001256\n",
      "Train Epoch:9 [48000/60000 (80%)]\tLoss:0.002155\n",
      "Train Epoch:9 [48640/60000 (81%)]\tLoss:0.004225\n",
      "Train Epoch:9 [49280/60000 (82%)]\tLoss:0.006840\n",
      "Train Epoch:9 [49920/60000 (83%)]\tLoss:0.000539\n",
      "Train Epoch:9 [50560/60000 (84%)]\tLoss:0.008180\n",
      "Train Epoch:9 [51200/60000 (85%)]\tLoss:0.003054\n",
      "Train Epoch:9 [51840/60000 (86%)]\tLoss:0.000571\n",
      "Train Epoch:9 [52480/60000 (87%)]\tLoss:0.000414\n",
      "Train Epoch:9 [53120/60000 (88%)]\tLoss:0.001597\n",
      "Train Epoch:9 [53760/60000 (90%)]\tLoss:0.165758\n",
      "Train Epoch:9 [54400/60000 (91%)]\tLoss:0.038905\n",
      "Train Epoch:9 [55040/60000 (92%)]\tLoss:0.003142\n",
      "Train Epoch:9 [55680/60000 (93%)]\tLoss:0.025576\n",
      "Train Epoch:9 [56320/60000 (94%)]\tLoss:0.003423\n",
      "Train Epoch:9 [56960/60000 (95%)]\tLoss:0.001012\n",
      "Train Epoch:9 [57600/60000 (96%)]\tLoss:0.000263\n",
      "Train Epoch:9 [58240/60000 (97%)]\tLoss:0.026587\n",
      "Train Epoch:9 [58880/60000 (98%)]\tLoss:0.006578\n",
      "Train Epoch:9 [59520/60000 (99%)]\tLoss:0.027086\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 设计一个卷积神经网络，并在其中使用ResNet模块，在MNIST数据集上实现10分类手写体数字识别。\n",
    "# 算一下每个数字的准确率\n",
    "# 超参数\n",
    "epochs = 10\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "learning_rate = 0.01\n",
    "# 这里的log_interval是指每隔多少个batch输出一次训练状态\n",
    "log_interval = 10\n",
    "random_seed = 1\n",
    "# 设置种子，为了使得结果可复现\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "# 从torchvision.datasets中加载MNIST数据集，并对数据进行标准化处理,参考网上\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.MNIST('./data/', train=True, download=True,\n",
    "                               transform=torchvision.transforms.Compose([\n",
    "                                   torchvision.transforms.ToTensor(),\n",
    "                                   # 这里设置均值和方差的值\n",
    "                                   torchvision.transforms.Normalize(\n",
    "                                       (0.1307,), (0.3081,))\n",
    "                               ])),\n",
    "    batch_size=batch_size_train, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.MNIST('./data/', train=False, download=True,\n",
    "                               transform=torchvision.transforms.Compose([\n",
    "                                   torchvision.transforms.ToTensor(),\n",
    "                                   torchvision.transforms.Normalize(\n",
    "                                       (0.1307,), (0.3081,))\n",
    "                               ])),\n",
    "    batch_size=batch_size_test, shuffle=True)\n",
    "\n",
    "\n",
    "# 定义残差模块\n",
    "class ResidualBlock(torch.nn.Module):\n",
    "    # 这里stride是指卷积的步长,保持输入输出的维度不变\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        # padding为1，保证输入输出的维度不变,bias=False,因为后面有BN层\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        # 这里的inplace=True是指将ReLU的输出直接覆盖到输入中，可以节省的显存，但是会影响收敛性\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = nn.Sequential()\n",
    "        # 这里的downsample是指如果输入输出的维度不一致，就需要对输入进行下采样，使得维度一致\n",
    "        # 原理是使用1*1的卷积核对输入进行卷积，同时步长为stride，这样就可以保证输入输出的维度一致\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 保存输入数据，采用恒等映射\n",
    "        identity = x\n",
    "\n",
    "        # 第一个卷积层\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        # 第二个卷积层\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        # 下采样匹配卷积操作的输入输出维度\n",
    "        identity = self.downsample(identity)\n",
    "\n",
    "        # 还原结果\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "# 构建包含ResidualBlock的网络，CNN\n",
    "class ResNet_CNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ResNet_CNN, self).__init__()\n",
    "        # mnist是灰度图，所以输入通道为1，输出通道为16,卷积核为3，步长为1，padding为1说明让输入输出维度不变\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.res1 = ResidualBlock(16, 16)\n",
    "        # 这里的stride=2,是因为输入输出维度不一致，需要下采样\n",
    "        self.res2 = ResidualBlock(16, 32, stride=2)\n",
    "        self.res3 = ResidualBlock(32, 64, stride=2)\n",
    "        self.res4 = ResidualBlock(64, 128, stride=2)\n",
    "        self.res5 = ResidualBlock(128, 128)\n",
    "        self.res6 = ResidualBlock(128, 256)\n",
    "        self.res7 = ResidualBlock(256, 256)\n",
    "\n",
    "        # 用一个自适应均值池化层将每个通道维度变成1*1\n",
    "\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        # 感觉数字特征比较简单，一个全连接层就够了\n",
    "        self.fc1 = nn.Linear(256, 7)\n",
    "        self.fc2 = nn.Linear(7, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.res1(x)\n",
    "        x = self.res2(x)\n",
    "        x = self.res3(x)\n",
    "        x = self.res4(x)\n",
    "        x = self.res5(x)\n",
    "        x = self.res6(x)\n",
    "        # x = self.res7(x)\n",
    "        # 64个通道，每个通道1*1，输出64*1*1\n",
    "        x = self.avg_pool(x)\n",
    "        # 将数据拉成一维\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# 实例化\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ResNet_CNN().to(device)\n",
    "# 定义损失函数\n",
    "loss_f = nn.CrossEntropyLoss()\n",
    "# 定义优化器\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "# 训练模型\n",
    "def train(epochs):\n",
    "    for epoch in range(epochs):\n",
    "        # 让BN层每一个mini-batch都要更新\n",
    "        model.train()\n",
    "        # 总损失\n",
    "        # train_loss = 0\n",
    "        # enumerate()函数用于将一个可遍历的数据对象组合为一个索引序列，同时列出数据和数据下标\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = loss_f(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # train_loss += loss.item()\n",
    "            # 每个mini-batch打印一次,loss.item()是一个mini-batch的平均损失\n",
    "            if batch_idx % log_interval == 0:\n",
    "                print('Train Epoch:{} [{}/{} ({:.0f}%)]\\tLoss:{:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                           100. * batch_idx / len(train_loader), loss.item()\n",
    "                ))\n",
    "    # 保存模型 state_dict()是一个字典，保存了网络中所有的参数\n",
    "    # 转换并保存为torch.jit的模型\n",
    "    example_input = torch.rand(1, 1, 28, 28).to(device)\n",
    "    traced_model = torch.jit.trace(model, example_input)\n",
    "    torch.jit.save(traced_model, \"traced_model.pt\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# 测试模型,每一个类别都要统计准确率，并统计总体准确率\n",
    "def test():\n",
    "    # 让BN层累计数据进行归一化\n",
    "    # 读取模型\n",
    "    model = torch.load('./traced_model.pt')\n",
    "    # model.eval()\n",
    "    # 总正确数\n",
    "    correct_all = 0\n",
    "    # 单个类别的正确数,这里用列表存储\n",
    "    correct_class = list(0. for i in range(10))\n",
    "    # 单个类别的总数\n",
    "    total_class = list(0. for i in range(10))\n",
    "    # 总测试数\n",
    "    total_all = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            output = model(images)\n",
    "            # torch.max()返回最大值和最大值的索引,这里要不要data?\n",
    "            _, predicted = torch.max(output, dim=1)\n",
    "            # 增加总测试数和总正确数\n",
    "            total_all += labels.size(0)\n",
    "            correct_all += (predicted == labels).sum().item()\n",
    "            # 增加单个类别的总数和正确数\n",
    "            for i in range(batch_size_test):\n",
    "                label = labels[i]\n",
    "                correct_class[label] += (predicted[i] == label).item()\n",
    "                total_class[label] += 1\n",
    "    # 打印每个类别的准确率\n",
    "    for i in range(10):\n",
    "        print('Accuracy of number{}:{:.2f}%'.format(\n",
    "            i, 100 * correct_class[i] / total_class[i]\n",
    "        ))\n",
    "    # 打印总体准确率\n",
    "    print('Accuracy of all:{:.2f}%'.format(100 * correct_all / total_all))\n",
    "\n",
    "\n",
    "# main\n",
    "if __name__ == '__main__':\n",
    "    epochs = 10\n",
    "    train(epochs)\n",
    "    # test()\n",
    "example_input = torch.rand(1, 1, 28, 28).to(device)  # 创建一个形状为 (1, 1, 28, 28) 的随机输入样本，并将其放置在指定的设备上（例如 GPU）\n",
    "traced_model = torch.jit.trace(model, example_input)  # 使用 torch.jit.trace 函数对模型进行跟踪（tracing），生成跟踪模型（traced model）\n",
    "torch.jit.save(traced_model, \"traced_model_15.pt\")  # 将跟踪模型保存到 \"traced_model.pt\" 文件中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd46d57-1f60-46e0-9cf2-3579a37ef7d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
